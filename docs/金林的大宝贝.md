# 面试题准备

## 项目介绍

离线数仓：当时我们从0到1搭建了离线数仓。选的是Hive on Spark架构。采集数据的话，我们当时日志数据是flume采集数据到kafka，然后再通过flume传输到hdfs上。业务数据就是从mysql通过sqoop传到hdfs上。在hdfs上我们对数据进行分层，分为ods+dwd+dws+ads层。ods层是存放原始数据，dwd层对数据进行清洗，去掉过期、重复、不完整的数据。dws对数据汇总。ads层为统计报表提供数据。



实时数仓：日志数据写入kafka，使用sparkstreaming读取。业务数据使用canal读取mysql的binlog文件，写入到kafka。然后使用sparkstreaming对数据进行处理，最后写入到hbase或者es。



小胖熊数据决策平台：主要负责报表和看板的开发，直接和业务对接，软件使用FineBI和FineReport实现。并且日常维护平台的权限，主要职责是BI。





## java和Scala的区别

| 区别                   | java                     | scala                           |
| ---------------------- | ------------------------ | ------------------------------- |
| 方法返回值             | 需要显示的return进行返回 | sacla可选，他是默认返回最后一行 |
| 接口                   | java支持接口             | 不支持接口，采用trait           |
| 类和方法修饰符的默认值 | protected                | public                          |
| 样例类                 |                          | 自动生成toString,equals方法     |



## linux常用命令

| 常用命令或参数         | 功能                                         | 额外说明                                                     |
| ---------------------- | -------------------------------------------- | ------------------------------------------------------------ |
| nohup                  | 免疫关闭session所发送的SIGHUP信号            | 常与&配合使用                                                |
| &                      | 免疫Ctrl + C发送的SIGINT信号                 | 常与nohup配合使用                                            |
| >/dev/null             | 将标准输出1重定向到`/dev/null`中             | /dev/null代表[linux](https://so.csdn.net/so/search?from=pc_blog_highlight&q=linux)的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞” |
| 2>&1                   | 重定向绑定                                   | 错误输出将会和标准输出输出到同一个地方                       |
| ps -ef/aux \|grep 进程 | 查看进程端口号                               |                                                              |
| grep -v                | 过滤不需要的进程                             | 其他参数：-i 忽略大小写 -E支持正则表达式                     |
| \|                     | 上一条命令的标准输出，作为下一条命令标准输入 | 示例：echo "--help" \| cat                                   |
| xargs                  | 上一条命令的标准输出，作为下一条命令的参数   | 示例：echo "--help" \| xargs cat                             |
| awk                    | 文本分析工具                                 | 示例：awk '{print $2 }' 返回分隔符的第二位，默认分隔符为空格，指定使用参数-F |
| ls -a                  | 显示隐藏文件                                 | 可显示后缀为.swp的文件                                       |
| rpm -qa                | 显示所有rpm的安装包                          |                                                              |
| rpm -e –nodeps         | 强制卸载安装包                               |                                                              |
| rpm -ivh               | 安装安装包                                   |                                                              |
| :g/^s*$/d              | 删除空行                                     | g ：全区命令 / ：分隔符 ^s*$ ：匹配空行，其中^表示行首，s表示空字符，包括空格和制表符，*重复0到n个前面的字符，$表示行尾。连起来就是匹配只有空字符的行，也就是空行。 /d ：删除该行 |
| %s/^\s*//g             | 删除行首空格                                 |                                                              |



## HDFS架构

NameNode：类似于Master

- 管理HDFS的名称空间
- 配置副本策略
- 管理数据块（Block)映射信息
- 处理客户端读写请求



DateNode:类似于Slave NameNode下达命令，DataNode执行实际的操作

- 存储实际的数据块
- 执行数据块的读写操作



Client:客户端

- 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传
- 与NameNode交互，获取文件的位置信息
- 与DataNode交互，读取或写入数据
- Client提供一些命令来管理HDFS，比如NameNode格式化
- Client可以通过一些命令来访问HDFS，比如HDFS增删改查的操作



Secondary NameNode

- 辅助NameNode,分担其工作量
- 紧急情况下，辅助恢复NameNode



数据块大小：128M

1. 数据块设置太小，增加寻址时间（程序寻找块的开始位置）
2. 块设置太大，磁盘传输数据的时间明显大于定位块位置时间，处理块数据时速度非常慢



## HDFS写流程

1. Client向NameNode请求上传文件，NameNode检查文件是否存在
2. NameNode返回是否可以上传
3. Client向NameNode请求上传第一个Block，请求返回DataNode
4. NameNode返回DataNode（dn1,dn2,dn3),表示这三个节点可以存储数据
5. Client向DataNode请求建立通道
6. DataNode逐级应答客户端
7. 客户端以Parket为单位，dn1每收到一个Parket就会传给dn2,dn2到dn3，**dn1每传一个packet就会放入一个应答队列等待应答**
8. 当第一个Block传输完成之后，客户端再次请求NameNode上传第二个Block(重复3-7)



## HDFS读流程

1. 客户端向NameNode请求下载文件，NameNode通过查询元数据，找到DataNode所在地址
2. 挑选DataNode（就近原则）服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位做校验）
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件



## Shuffle

Shuffle机制过程

1. maptask将map()方法输出的k,v对存放到环形缓冲区
2. 当环形缓冲区达到溢写比例时，一般为80%，从内存溢写文件到磁盘
3. 用快速排序对缓冲区里的数据进行排序，先按照分区编号Partiton排序，然后按照key排序
4. 将所有的溢出文件通过归并排序合并成一个大的溢出文件
5. reduceTask根据分区号从各个Maptask上取相应的数据
6. reduce会取到同一个分区来自不同MapTask的结果文件，并通过归并排序对这些文件再次进行排序
7. 生成大文件之后，shuffe结束，进入reduceTask的逻辑运算过程



## Yarn架构

ResouceManager

- 处理客户端请求
- 监控NodeManager
- 启动或监控ApplicationMaster
- 资源的分配与调度



NodeManager

- 管理单个节点上的资源
- 处理来自ResourManager的命令
- 处理来自ApplicationMaster的命令



ApplicationMaster

- 负责数据的切分(根据提交到HDFS的切片信息划分)
- 为应用程序申请资源并分配给内部的任务
- 任务的监控与容错



Container

- Container是Yarn中资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等



## Yarn工作机制

作业提交过程

1. Client向整个集群提交MapReduce作业
2. Client向RM申请一个作业id
3. RM给Client返回该job资源提交的路径和作业id
4. Client提交jar包、切片信息和配置文件到指定的资源提交路径
5. Client提交完资源后，向RM申请运行MrAppMaster



作业初始化

1. 当RM收到Client请求后，将job添加到容量调度器中
2. 某一个空闲的NM领取到Job
3. 该NM创建Container，并产生MRAppmaster
4. 下载Clinet提交的资源到本地



任务分配

1. MRAppMaster向RM申请运行多个MapTask任务资源
2. RM将运行MapTask任务分配给另外两个NodeManager,另两个NodeManager分别领取任务并创建容器



任务运行

1. MRAppMaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManger分别启动MapTask,MapTask对数据分区排序
2. MRAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask
3. ReduceTask向MapTask获取相应分区的数据
4. 程序运行完毕，MR会向RM申请注销自己



## flume三大组件

### Source

source组件用来收集数据的，可以收集各种类型和格式的日志数据

​	包括avro,thrift,taildirsource

​	taildirsource优点：支持断点续传、多目录

### Channel

可以对数据进行缓存，可以存在memory或者file中

| memory channel | 基于内存        | 效率高、可靠性低 |
| -------------- | --------------- | ---------------- |
| file channel   | 基于磁盘        | 效率低、可靠性高 |
| kafka channel  | 基于kafka的磁盘 | 效率高、可靠性高 |

选用memory channel

### Sink

用于把数据发送到目的地的组件

​	包括Hdfs、Logger、avro、thrift、Hbase





## flume拦截器

### JSON格式拦截器

阶段：flume至kafka,source至channel

作用：进行ETL清洗，过滤JSON格式不完整的数据

代码

```
package com.xpx.flume.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }
}
```



### 时间戳拦截器

阶段：flume至hdfs

作用：解决零点漂移的问题，避免flume上传到hdfs时采用linux默认系统时间产生误差，需要取到日志生成的时间

解决方案：将event的body信息转化为json,获取其中时间戳的值，放入event的headers信息中，headers名字为timestamp

代码

### 自定义拦截器步骤

1. 实现Interceptor接口，重写4个方法
2. intercept方法处理单个event
3. Intercept方法处理event集合
4. 将event的body信息转化为json,获取其中时间戳的值，放入event的headers信息中，headers名字为timestamp
5. 增加静态内部类返回一个拦截器对象

代码

```java
package com.xpx.flume.interceptor;


import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;

public class TimestampInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        Map<String, String> headers = event.getHeaders();
        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);
        JSONObject jsonObject = JSONObject.parseObject(log);
        String ts = jsonObject.getString("ts");
        headers.put("timestamp",ts);
        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        for (Event event : list) {
            intercept(event);
        }
        return list;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new TimestampInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```



## hdfs小文件问题

小文件的危害

- 存储：占用namenode内存，每个小文件会占用150个字节
- 计算：默认切片规则为一个文件切一次，一个小文件对应一个maptask,一个maptask占用一个内存



①在flume的配置文件中配置参数

| 参数              | 解释                                                         |
| ----------------- | ------------------------------------------------------------ |
| hdfs.rollInterval | 文件创建超xxx秒时会滚动生成新文件                            |
| hdfs.rollSize     | 文件超过xxxM时会滚动生成新文件                               |
| hdfs.rollCount    | event个数达到xxx时会滚动生成新的文件（设置为0则永远不根据事件的数量滚动） |



vim kafka-flume-hdfs.conf

```shell
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = xpx101:9092,xpx102:9092,xpx103:9092
a1.sources.r1.kafka.topics=haha
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.xpx.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
#磁盘数据在内存中索引落盘的位置
a1.channels.c1.checkpointDir = /opt/xpx/module/flume/checkpoint/behavior1
#数据备份的磁盘位置
a1.channels.c1.dataDirs = /opt/xpx/module/flume/data/behavior1/


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
#前缀
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

#控制生成的小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

```



②设置 set hive.input.format 

hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;（在map端合并小文件）

但是在有LZO索引文件时会导致LZO索引文件丢失，此时需要取消合并小文件

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;





## kafka

kafka的优势

消息队列：解耦、缓冲、削峰、灵活性高、异步

基于发布/订阅模式 相比点对点（一个消息只能被一个消费者消费）处理的更快

发布/订阅模式也分为队列推数据和消费者拉取数据

队列推数据：可能产生推数据过快，消费者来不及消费的情况，崩掉的情况，也有可能存在资源浪费的情况

消费者主动拉取缺点：consumer不断向topic进行轮询，造成资源的浪费



同一个消费者组的不同消费者不能够消费同一个主题的同一个分区



zookeeper作用：存储kafka的集群信息，存储kafka消费者的消费位置信息 0.9版本之后记录在系统主题当中，不存在zookeeper是因为consumer在消费的同时，和集群交互同时还要和zookeeper交互，效率太低

kafka消息存储在磁盘，默认存7天

### kafka broker

分区的好处：

提高负载

增加并行度

### kafka生产者分区规则

指定分区

根据key的哈希值取余分区数

round-robin轮询

kafka 常用命令

查看topic

```
 bin/kafka-topics.sh  --zookeeper xpx101:2181/kafka --list
```

创建topic

```
bin/kafka-topics.sh --zookeeper xpx101:2181/kafka --create --replication-factor 3 --partitions 3 --topic first
```

### ISR

ISR队列的作用：因为kafka副本同步策略选择的是所有的副本全部同步，但是可能存在部分机器损坏导致阻塞，通过设置最大等待时间的参数（默认10秒），超过这个时间就剔除ISR队列

follower被踢出ISR之后如何重新加入：将自身高于HW的部分截取掉，当自己的LEO到达该分区的HW后重新加入ISR

### ack级别

0：直接发  最容易丢数据

1：leader接收完  较可能丢数据

-1：leader和follower（ISR队列里的follower）都接收完  保证不丢数据，可能数据重复

### 保证数据一致性

LEO：每个副本的最后一个offset

HW：ISR副本中最小的LEO

消费端一致性：comsumer只能消费到HW

存储端一致性：leader挂掉时，新leader以外的ISR队列成员将HW以外的截取掉，统一向新leader同步

### Excatly Once 精准一次性消费

Excatly Once：ack=-1+幂等性（开启参数即可）

幂等性原理：在produce初始化时会生成pid，这个生产者的消息会附带序列化信息，broker端会有一个<pid,partition,seq>的缓存，一个信息只有持久化一次

因为produce重启会生成新的pid，所以不支持跨会话跨分区的精准一次

### 消费者分区分配规则

消费者数量变化时触发

RoundRobin:轮询  所有主题+分区作为整体，消费者之间资源最多只差一个，无法满足一个消费者组只消费一个主题的需求

Range:范围 每个主题作为一个整体，可以指定消费者组消费指定分区，但是可能消费者资源分配不均

默认range

### Offset维护

由主题+分区+消费者组确定一个offset

代码测试

创建主题

```
bin/kafka-topics.sh --create --topic bigdata --zookeeper xpx101:2181/kafka --partitions 2 --replication-factor 2
```

启动生产者

```
bin/kafka-console-producer.sh --broker-list xpx101:9092 --topic bigdata
```

启动消费者

```
bin/kafka-console-consumer.sh --bootstrap-server xpx101:9092 --topic bigdata --consumer-property group.id=test1-group --from-beginning
```

查看消费者的offset信息

```
./kafka-consumer-groups.sh --bootstrap-server xpx101:9092 --describe --group test1-group
```

zookeeper查看,新版kafka无法查看到controller节点下的信息

```
ls /kafka
get /kafka/controller
```

Kafka落盘仍高效的原因

分布式，有分区，并行度高

顺序写磁盘，减少寻址时间

使用了零拷贝技术，减少了传输过程，文件-操作系统-到用户层 简化为文件-操作系统-文件

### Controller作用

Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群broker的上下线，所有 topic 的分区副本分配和 leader选举

### kafka组件顺序

拦截器-序列化-分区器

### Producer事务

客户端提供唯一的事务ID（Transaction ID），将pid和事务ID绑定，重启之后根据Transaction ID获取pid,实现跨分区跨回话的精准一次

消费者事务则是消费一次

## Spark算子

transformation算子

| map          | 映射                 |
| ------------ | -------------------- |
| mapPartitons | 以分区为单位进行映射 |
| flatMap      | 扁平化               |
| groupBy      | 分组                 |
| filter       | 过滤                 |
| distinct     | 去重                 |
| repartiton   | 重新分区             |
| sortBy       | 排序                 |

action算子

| reduce      | 聚合                             |
| ----------- | -------------------------------- |
| collect     | 以数组形式返回数据集             |
| count       | 返回RDD中元素个数                |
| foreach     | 遍历RDD的每一个元素              |
| take        | 返回前n个元素组成的数组          |
| takeOrdered | 返回RDD排序后前n个元素组成的数组 |



## Spark数据倾斜

| 使用场景                                                     | 解决方案                                                     | 优点                            | 缺点                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------- | --------------------------------------------------------- |
| Hive表本身数据就不均匀，而且业务场景需要频繁去用spark对Hive表执行分析操作 | 可以提前在Hive中就做聚合或者join操作                         | 简单便捷、完全规避掉shuffle阶段 | 治标不治本，HiveETL中还是会发生数据倾斜                   |
| 对RDD执行聚合类shuffle算子或者在SparkSql中使用group by进行分组聚合时 | 第一次局部聚合，先给key打上随机数前缀，接着执行聚合操作，将随机数前缀去掉，再次进行聚合操作 | 对于聚合类shuffle操作效果是很好 | 适用范围比较窄，如果是join类的shuffle操作就不行           |
| 大小表join                                                   | 广播小表                                                     | 简单便捷、完全规避掉shuffle阶段 | 适用场景小，只适合大小表join，而且会比较消耗内存，容易OOM |
| 两个大表join，一张倾斜key的数量比较多，另一张key分布均匀     | 将正常表key扩容N倍，对倾斜表中数据每一个都打上N以内的随机数，然后join | 对Join类型的数据基本都可以处理  | 只是缓解数据倾斜，而且对内存资源要求很高                  |



### 二次聚合

加随机数打散

```scala
object SkewAggregationTuning {
  def main( args: Array[String] ): Unit = {

    val sparkConf = new SparkConf().setAppName("SkewAggregationTuning")
      .set("spark.sql.shuffle.partitions", "36")
//      .setMaster("local[*]")
    val sparkSession: SparkSession = InitUtil.initSparkSession(sparkConf)

    sparkSession.udf.register("random_prefix", ( value: Int, num: Int ) => randomPrefixUDF(value, num))
    sparkSession.udf.register("remove_random_prefix", ( value: String ) => removeRandomPrefixUDF(value))


    val sql1 =
      """
        |select
        |  courseid,
        |  sum(course_sell) totalSell
        |from
        |  (
        |    select
        |      remove_random_prefix(random_courseid) courseid,
        |      course_sell
        |    from
        |      (
        |        select
        |          random_courseid,
        |          sum(sellmoney) course_sell
        |        from
        |          (
        |            select
        |              random_prefix(courseid, 6) random_courseid,
        |              sellmoney
        |            from
        |              sparktuning.course_shopping_cart
        |          ) t1
        |        group by random_courseid
        |      ) t2
        |  ) t3
        |group by
        |  courseid
      """.stripMargin


    val sql2=
      """
        |select
        |  courseid,
        |  sum(sellmoney)
        |from sparktuning.course_shopping_cart
        |group by courseid
      """.stripMargin

    sparkSession.sql(sql1).show(10000)


//    while(true){}
  }


  def randomPrefixUDF( value: Int, num: Int ): String = {
    new Random().nextInt(num).toString + "_" + value
  }

  def removeRandomPrefixUDF( value: String ): String = {
    value.toString.split("_")(1)
  }
}
```



## Spark提交参数

| executor-cores  | executor 的最大核数                | 3-6   一般设置为4 |
| --------------- | ---------------------------------- | ----------------- |
| num-executors   | 每个节点的executor 数* work 节点数 | (28/4)* 7 = 49G   |
| executor-memory | yarn内存数/每个节点的executor数    | 100G/7 = 14G      |



## Spark内存分布

| Other 内存    | 自定义数据结构*每个 Executor 核数         | 默认占可用内存40% |
| ------------- | ----------------------------------------- | ----------------- |
| Storage 内存  | 广播变量+ cache/Executor 数量             | 默认占统一内存50% |
| Executor 内存 | 每个 Executor 核数* （数据集大小/并行度） | 默认占统一内存50% |
| 统一内存      | Storage 内存+Executor 内存                | 默认占可用内存60% |



Executor示例：4 * （100G/200) = 2G



## Spark 资源调优

### 内存优化

rdd缓存采用kryo序列化，大幅减少资源占用

df、ds设置存储级别为 MEMORY_AND_DISK_SER（设不设置区别不大）

使用默认级别MEMORY_ONLY 是对 CPU 的支持最好的。但是序列化缓存可以让体积更小，那么当 yarn 内存资源不充足情况下可以考虑使用 MEMORY_ONLY_SER 配合 kryo 使用序列化缓存



### CPU优化

一般会将并行度（task 数）设置成并发度

（vcore 数）的 2 倍到 3 倍

并发度 = num-executors * executor-cores



## SparkSQL语法优化

SparkSQL 在整个执行计划处理的过程中，使用了 Catalyst 优化器

### RBO优化

#### 谓词下推

看执行计划，优化后的逻辑计划能先过滤的就先过滤

先filter再join，inner join对两张表都进行了过滤

#### 列裁剪

列剪裁就是扫描数据源的时候，只读取那些与查询相关的字段

#### 常量替换

例如：用5代替2+3



### CBO优化 

先对表、列进行数据的统计

开启cbo后可能会自动广播小表



## 广播小表

默认小于10M自动广播

强制广播

```sql
    val sqlstr1 =
      """
        |select /*+  BROADCASTJOIN(sc) */
        |  sc.courseid,
        |  csc.courseid
        |from sale_course sc join course_shopping_cart csc
        |on sc.courseid=csc.courseid
      """.stripMargin
```



## SMB JOIN

SMB JOIN 是 sort merge bucket 操作，需要进行分桶，首先会进行排序，然后根据 key 值合并，把相同 key  的数据放到同一个 bucket  中（按照 key  进行 hash）。分桶的目的其实就是把大表化成小表。相同 key 的数据都在同一个桶中之后，再进行 join 操作，那么在联合的时候就会大幅度的减小无关项的扫描。

条件：

（1） 两表进行分桶，桶的个数必须相等

（2）两边进行 join 时，join 列=排序列=分桶列



## Typora快捷键

有序列表：1.+TAB

无序列表：*+TAB