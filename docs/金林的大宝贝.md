# 项目介绍

离线数仓：当时我们从0到1搭建了离线数仓。选的是Hive on Spark架构，使用spark作为计算引擎。采集数据的话，我们当时日志数据是flume采集数据到kafka，然后再通过flume传输到hdfs上。业务数据就是从mysql通过sqoop传到hdfs上。在hdfs上我们对数据进行分层，分为ods+dwd+dws+ads层。ods层是存放原始数据，dwd层对数据进行清洗，去掉过期、重复、不完整的数据。dws对数据汇总。ads层为统计报表提供数据。最后数据还是通过sqoop导出到mysql。



实时数仓：接下来是实时数仓，日志数据通过flume采集到kafka,业务数据通过maxwell读取mysql的binlog文件采集到kafka，形成ods层.之后对数据进行分层，分为dim层、dwd层、dws层、ads层，dim层的数据存入hbase,dwd层是存入kafka的不同主题，dws层写入clickhouse，ads层调用接口对clickhouse进行查询。



小胖熊数据决策平台：职责是服务中台，和业务以及一些业务部门的数据分析人员对接。会维护BI平台的日常运转以及看板和报表的开发，主要是通过FineBI和FineReporter实现。



# java和Scala的区别

| 区别                   | java                     | scala                           |
| ---------------------- | ------------------------ | ------------------------------- |
| 方法返回值             | 需要显示的return进行返回 | sacla可选，他是默认返回最后一行 |
| 接口                   | java支持接口             | 不支持接口，采用trait           |
| 类和方法修饰符的默认值 | protected                | public                          |
| 样例类                 |                          | 自动生成toString,equals方法     |



# linux常用命令

| 常用命令或参数     | 功能                                         | 额外说明                                                     |
| ------------------ | -------------------------------------------- | ------------------------------------------------------------ |
| pwd                | 显示当前工作目录                             |                                                              |
| mkdir              | 创建目录                                     |                                                              |
| cd                 | 切换目录                                     |                                                              |
| cat                | 查看文件                                     |                                                              |
| find               | 查找文件                                     |                                                              |
| vim                | 编辑文件                                     |                                                              |
| kill -9            | 终止进程                                     |                                                              |
| ps -ef\|grep 进程  | 查看进程端口号                               |                                                              |
| ps -aux\|grep 进程 | 查看进程端口号                               |                                                              |
| ls -a              | 显示隐藏文件                                 | 可显示后缀为.swp的文件                                       |
| nohup              | 免疫关闭session所发送的SIGHUP信号            | 常与&配合使用                                                |
| \|                 | 上一条命令的标准输出，作为下一条命令标准输入 | 示例：echo "--help" \| cat                                   |
| xargs              | 上一条命令的标准输出，作为下一条命令的参数   | 示例：echo "--help" \| xargs cat                             |
| awk                | 文本分析工具                                 | 示例：awk '{print $2 }' 返回分隔符的第二位，默认分隔符为空格，指定使用参数-F |
| rpm -qa            | 显示所有rpm的安装包                          |                                                              |
| &                  | 免疫Ctrl + C发送的SIGINT信号                 | 常与nohup配合使用                                            |
| rpm -ivh           | 安装安装包                                   |                                                              |
| >/dev/null         | 将标准输出1重定向到`/dev/null`中             | /dev/null代表[linux](https://so.csdn.net/so/search?from=pc_blog_highlight&q=linux)的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞” |
| 2>&1               | 重定向绑定                                   | 错误输出将会和标准输出输出到同一个地方                       |
| grep -v            | 过滤不需要的进程                             | 其他参数：-i 忽略大小写 -E支持正则表达式                     |
| rpm -e –nodeps     | 强制卸载安装包                               |                                                              |
| :g/^s*$/d          | 删除空行                                     | g ：全区命令 / ：分隔符 ^s*$ ：匹配空行，其中^表示行首，s表示空字符，包括空格和制表符，*重复0到n个前面的字符，$表示行尾。连起来就是匹配只有空字符的行，也就是空行。 /d ：删除该行 |
| %s/^\s*//g         | 删除行首空格                                 |                                                              |



# Hadoop分布式系统

## HDFS架构

NameNode：类似于Master

- 管理HDFS的名称空间
- 配置**副本**策略
- 管理数据块（Block)映射信息
- 处理客户端读写请求



DateNode:类似于Slave NameNode下达命令，DataNode执行实际的操作

- **存储**实际的数据块
- 执行数据块的**读写**操作



Client:客户端

- 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传
- 与NameNode交互，获取文件的位置信息
- 与DataNode交互，读取或写入数据
- Client提供一些命令来管理HDFS，比如NameNode格式化
- Client可以通过一些命令来访问HDFS，比如HDFS增删改查的操作



Secondary NameNode

- 辅助NameNode,分担其工作量
- 紧急情况下，辅助恢复NameNode



数据块大小：128M

1. 数据块设置太小，增加寻址时间（程序寻找块的开始位置）
2. 块设置太大，磁盘传输数据的时间明显大于定位块位置时间，处理块数据时速度非常慢



## HDFS写流程

1. Client向NameNode请求上传文件，NameNode检查文件是否存在
2. NameNode返回是否可以上传
3. Client向NameNode请求上传第一个Block，请求返回DataNode
4. NameNode返回DataNode（dn1,dn2,dn3),表示这三个节点可以存储数据
5. Client向DataNode请求建立通道
6. DataNode逐级应答客户端
7. 客户端以Parket为单位，dn1每收到一个Parket就会传给dn2,dn2到dn3，**dn1每传一个packet就会放入一个应答队列等待应答**
8. 当第一个Block传输完成之后，客户端再次请求NameNode上传第二个Block(重复3-7)



## HDFS读流程

1. 客户端向NameNode请求下载文件，NameNode通过查询元数据，找到DataNode所在地址
2. 挑选DataNode（就近原则）服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位做校验）
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件



## Shuffle

Shuffle机制过程

1. maptask将map()方法输出的k,v对存放到环形缓冲区
2. 当环形缓冲区达到溢写比例时，一般为80%，从内存溢写文件到磁盘
3. 用快速排序对缓冲区里的数据进行排序，先按照分区编号Partiton排序，然后按照key排序
4. 将所有的溢出文件通过归并排序合并成一个大的溢出文件
5. reduceTask根据分区号从各个Maptask上取相应的数据
6. reduce会取到同一个分区来自不同MapTask的结果文件，并通过归并排序对这些文件再次进行排序
7. 生成大文件之后，shuffe结束，进入reduceTask的逻辑运算过程



## Yarn架构

ResouceManager

- 处理客户端请求
- 监控NodeManager
- 启动或监控ApplicationMaster
- 资源的分配与调度



NodeManager

- 管理单个节点上的资源
- 处理来自ResourManager的命令
- 处理来自ApplicationMaster的命令



ApplicationMaster

- 负责**数据的切分**(根据提交到HDFS的切片信息划分)
- 为应用程序**申请资源**并分配给内部的任务
- 任务的监控与容错



Container

- Container是Yarn中资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等



## Yarn工作机制

作业提交过程

1. Client向整个集群提交MapReduce作业
2. Client向RM申请一个作业id
3. RM给Client返回该job资源提交的路径和作业id
4. Client提交**jar包、切片信息和配置文件**到指定的资源提交路径
5. Client提交完资源后，向RM申请运行MrAppMaster



作业初始化

1. 当RM收到Client请求后，将job添加到容量调度器中
2. 某一个空闲的NM领取到Job
3. 该NM创建Container，并产生MRAppmaster
4. 下载Clinet提交的资源到本地



任务分配

1. MRAppMaster向RM申请运行多个MapTask任务资源
2. RM将运行MapTask任务分配给另外两个NodeManager,另两个NodeManager分别领取任务并创建容器



任务运行

1. MRAppMaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManger分别启动MapTask,MapTask对数据分区排序
2. MRAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask
3. ReduceTask向MapTask获取相应分区的数据
4. 程序运行完毕，MR会向RM申请注销自己



# flume

## Source

source组件用来收集数据的，可以收集各种类型和格式的日志数据

 包括avro,thrift,taildirsource

 taildirsource优点：支持断点续传、多目录

## Channel

可以对数据进行缓存，可以存在memory或者file中

| memory channel | 基于内存        | 效率高、可靠性低 |
| -------------- | --------------- | ---------------- |
| file channel   | 基于磁盘        | 效率低、可靠性高 |
| kafka channel  | 基于kafka的磁盘 | 效率高、可靠性高 |

选用memory channel

## Sink

用于把数据发送到目的地的组件

 包括Hdfs、Logger、avro、thrift、Hbase



# Zookeeper

## 一、zookeeper入门

### 1.1 概述

```mysql
-- 理解
1) Apache一个分布式项目；
2）是一个基于观察者模式设计的分布服务管理框架，负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦数据发生变化，则zookeeper通知观察者。

-- zookeeper = 文件系统 + 通知机制。
```

### 1.2 zookeeper特点

```mysql
-- 特点
1)  每一节点都有一个不重复的myid标识zookeeper集群；
2） 一个领导者（leader） 和 n 个追随者 （follower）组成的集群；
3） 集群只要有半数以上的节点存活就能对外提供服务，即使是在leader失败的情况下；-- 半数机制
4） 根据半数机制，可知一般搭建奇数台的机器是有优势的；
  假设是4台，半数为2，故障一台，还有3台，3 > 2 ,所以继续提供服务，再故障一台，剩下2台， 2 > 2 ,错误，此时，不再对外提供服务；
  假设是 3台，半数为 1.5 ，故障1台，还有2台， 2 > 1.5 , 所以继续提供服务，再故障一台，剩下1台， 1 > 1.5 ,错误，此时不再对外提供服务。
  综上，4台机器，允许故障1台，3台机器，也允许故障一台，所以奇数台机器有优势。
5） 全局数据一致：所有节点的数据是保持一致的，所以客户端无论连接到哪台机器，获取的数据都是一样的； -- 基于zab机制；
6） 更新请求按照顺序执行；
7） 数据更新的原子性：要么成功，要么失败；
8） 实时性： 在一定时间范围内，client能够获得最新的数据。
```

### 1.3 应用场景

```
1） 统一命名服务；
2） 统一配置管理；
3） 服务器节点动态上下线；
4） 软负载均衡。
```

## 二、zookeeper内部原理

### 2.1  节点类型

```mysql
分为持久性和短暂性节点。
1） 持久性：客户端与服务器断开连接以后，创建的节点不删除；
  -- 分为带序号的持久性节点和不带序号的持久性节点。
2） 短暂性：客户端与服务器断开连接以后，创建的节点删除；
  -- 分为带序号的短暂性节点和不带序号的短暂性节点。
  
-- 如何理解带序号的呢？
1） 在zk分布系统中，顺序号用于为所有事物进行全局排序，这样客户端根据顺序号推测事件的顺序。
2） 顺序号是指当前节点下的节点数量，不可重复使用。如之前已经创建了一个节点，但是现在将其进行删除，再创建一个节点，顺序号是往后进行累加。

-- 说明
1） 短暂节点下不能创建子节点；
2） 一个节点包含该节点的具体存储的内容和子节点的元数据信息。
```

### 2.2  监听器原理（重点）

```mysql
 监听原理详解：
 1） 首先有一个main（）线程；
 2） 在main线程中创建zookeeper客户端，这时就会有两个线程，一个负责网络通信（connet），一个负责监听（listener）；
 3） 通过connect线程将注册的监听事件发送给zookeeper；
 4） 在zookeeper的注册监听器列表中将注册的监听事件添加到列表中；
 5） zookeeper监听到有数据或者是路径发生变化时，就会将这个消息发送到listener线程；
 6） listener线程内部调用process方法（）；
```

### 2.3  选举机制（重点）

核心概念

- myid 编号越大在选择算法中的权重越大，初始化启动就是根据服务器id比较
- zxid 事务ID 服务器中存放的事务ID，值越大说明数据越新，在选举算法中数据越新权重越大
- 选举状态
  - LOOKING，竞选状态
  - FOLLOWING，随从状态，同步leader状态，参与投票。
  - OBSERVING，观察状态,同步leader状态，不参与投票。
  - LEADING，领导者状态



**选举机制触发场景**

**服务器初始化启动（以mid作为依据）**

1. 服务器1启动

 发起一次选举，服务器1投自己一票，此时服务器1票数一票，不够半数以上（3票），选举无法完成。

 投票结果：服务器1为1票。

 服务器1状态保持为LOOKING。

2. 服务器2启动

 发起一次选举，服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2。

 投票结果：服务器1为0票，服务器2为2票。

 服务器1，2状态保持LOOKING

3. 服务器3启动

 发起一次选举，服务器1、2、3先投自己一票，然后因为服务器3的id最大，两者更改选票投给为服务器3；

 投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），**服务器3当选****Leader**。

 服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING。

4. 服务器4启动

 发起一次选举，此时服务器1，2，3已经不是LOOKING 状态，不会更改选票信息。交换选票信息结果：服务器3为3票，服务器4为1票。此时服务器4服从多       数，更改选票信息为服务器3。

 服务器4并更改状态为FOLLOWING。

5. 服务器5启动

 与服务器4一样投票给3，此时服务器3一共5票，服务器5为0票。服务器5并更改状态为FOLLOWING。

最终的结果：

服务器3是 Leader，状态为 LEADING；其余服务器是 Follower，状态为 FOLLOWING。

**服务器运行期间leader故障(以zxid为依据)**

```mysql
总结：选举机制由节点启动的顺序、myid、数据的zxid、服务器的数量有关。
大致顺序为：
1）zxid大的当选（99%情况下都是相等的）；
2）根据节点启动的顺序，比较myid，在未到达半数的服务器数量以前，所有节点的票都将投给myid大的服务器，一旦到达了半数以上的服务器被启动（此时可以对外提供服务）时，myid最大的节点当选leader，其余的服务器为follower。
```





## 三、zookeeper实战部署

### 3.1 客户端命令行操作

|      命令       |               解释                |
| :-------------: | :-------------------------------: |
|      help       |        显示所有的操作指令         |
|     ls path     | 使用ls命令来查看当前znode的子节点 |
|   ls -s path    |     查看当前目录下的详细信息      |
| create 创建节点 |                                   |
|    get path     |           获得节点的值            |
|    set path     |         设置节点具体的值          |
|     delete      |             删除节点              |
|    deleteall    |           递归删除节点            |



# Kafka

## 定义

Kafka是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域

## kafka基础架构

![image-20220919064037186](https://gitee.com/it-wont-work/typora-cloud-map-library/raw/master/img/image-20220919064037186.png)



1）Producer ：消息生产者，就是向kafka broker发消息的客户端；

2）Consumer ：消息消费者，向kafka broker取消息的客户端；

3）Consumer Group （CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者**。

4）Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。

5）Topic ：可以理解为一个队列，生产者和消费者面向的都是一个topic**；

6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition**，每个partition是一个有序的队列；

7）Replica：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个**leader**和若干个**follower**。

8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。

9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的leader。



![image-20220919063956791](https://gitee.com/it-wont-work/typora-cloud-map-library/raw/master/img/image-20220919063956791.png)



## kafka生产者分区规则

指定分区

根据key的哈希值取余分区数

round-robin轮询



## 消费者分区分配规则

消费者数量变化时触发

RoundRobin:轮询  所有主题+分区作为整体，消费者之间资源最多只差一个，无法满足一个消费者组只消费一个主题的需求

Range:范围 每个主题作为一个整体，可以指定消费者组消费指定分区，但是可能消费者资源分配不均

默认range



## ISR

ISR队列的作用：因为kafka副本同步策略选择的是所有的副本全部同步，但是可能存在部分机器损坏导致阻塞，通过设置最大等待时间的参数（默认10秒），超过这个时间就剔除ISR队列

follower被踢出ISR之后如何重新加入：将自身高于HW的部分截取掉，当自己的LEO到达该分区的HW后重新加入ISR



## ack级别

0：直接发  最容易丢数据

1：leader接收完  较可能丢数据

-1：leader和follower（ISR队列里的follower）都接收完  保证不丢数据，可能数据重复



## 保证数据一致性

LEO：每个副本的最后一个offset

HW：ISR副本中最小的LEO

消费端一致性：comsumer只能消费到HW

存储端一致性：leader挂掉时，新leader以外的ISR队列成员将HW以外的截取掉，统一向新leader同步



## Excatly Once 精准一次性消费

Excatly Once：ack=-1+幂等性（开启参数即可）

幂等性原理：在produce初始化时会生成pid，这个生产者的消息会附带序列化信息，broker端会有一个<pid,partition,seq>的缓存，一个信息只有持久化一次

因为produce重启会生成新的pid，所以不支持跨会话跨分区的精准一次



## **Kafka消息数据积压** 

1）如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）

2）如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间<生产速度），使处理的数据小于生产的数据，也会造成数据积压



## **Kafka参数优化**

- Broker参数配置（server.properties）

```
1、日志保留策略配置
# 保留三天，也可以更短 （log.cleaner.delete.retention.ms）
log.retention.hours=72
2、Replica相关配置
default.replication.factor:1 默认副本1个
3、网络通信延时
replica.socket.timeout.ms:30000 #当集群之间网络不稳定时,调大该参数
replica.lag.time.max.ms= 600000# 如果网络不好,或者kafka集群压力较大,会出现副本丢失,然后会频繁复制副本,导致集群压力更大,此时可以调大该参数
```

- Producer优化（producer.properties）

```
compression.type:none                 gzip  snappy  lz4  
#默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力。
```

- kafka内存tiaozheng

默认内存1个G，生产环境尽量不要超过6个G

```
export KAFKA_HEAP_OPTS="-Xms4g -Xmx4g"
```



# Hive

## Hive架构原理

<img src="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200720000427.png" alt="image-20200720000427660" style="zoom: 67%;" />

```sql
-- 1. 用户接口：Client
  CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）
-- 2. 元数据：Metastore
  元数据包括：
     a、表名
     b、表所属的数据库（默认是default）
     c、表的拥有者
     d、列/分区字段
     e、表的类型（是否是外部表）、
     f、表的数据所在目录等；
  '默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore'
-- 3. Hadoop
  使用HDFS进行存储，使用MapReduce进行计算。
-- 4. 驱动器：Driver
  1. '解析器'（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，
     比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。
    2. '编译器'（Physical Plan）：将AST编译生成逻辑执行计划。
    3. '优化器'（Query Optimizer）：对逻辑执行计划进行优化。
    4. '执行器'（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。
```



## Hice与数据库比较

```sql
-- 1. 查询语言
    hive有类似sql的hql查询语言
-- 2. 数据更新
  1. hive针对数据仓库而设计，适合读多写少的场景
  2. mysql的数据需要经常进行修改。
-- 3.  执行延迟
  1. hive没有索引 + 基于mr计算，延迟性高；
  2. 这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive的并行计算显然能体现出优势
-- 4. 数据规模
    1. 支持大数据规模的数据
```



## 内部表与外部表

区别

```sql
1.管理表：也称内部表，当删除管理表时，hdfs中的数据和mysql中的元数据均会被删除 -- 控制表的生命周期
2.外部表：当删除管理表时，hdfs中的数据不会被删除，mysql中的元数据会被删除  -- 不能控制表的生命周期
在实战过程中，我们一般都是使用外部表。
```

## 函数 （重点）

### 常用函数

日期函数：

```mysql
1） unix_timestamp : 返回当前或指定的时间戳；
SELECT  unix_timestamp("2020-05-02 11:22:00"); ==>1588418520
2） from_unixtime : 将时间戳转化为日期格式
SELECT FROM_unixtime(1588418520); ==> 2020-05-02 11:22:00
3) current_date : 当前日期
4）current_timestamp: 当前日期 + 时间；
5）to_date : 获取日期部分
6）year/month/day/hour/minute/second() : 获取年、月、日、小时、分、秒
7）weekofyear(): 当前时间是一年中的第几周
8）dayofmonth(): 当前时间是一个月中的第几天
9）months_between() : 两个日期间的月份
10) datediff() : 两个日期相差的天数
11) add_months：日期加减月
12) date_add：日期加天数
13) date_sub：日期减天数
14) last_day: 日期的当月的最后一天
```

取整函数

```mysql
1) round： 四舍五入
2) ceil：  向上取整
3) floor： 向下取整
```

字符串函数

```mysql
1）upper： 转大写
2）lower： 转小写
3）length： 长度
4）trim：  前后去空格
5）lpad： 向左补齐，到指定长度
6）rpad：  向右补齐，到指定长度
7）regexp_replace： SELECT regexp_replace('100-200', '(\\d+)', 'num') ；
  使用正则表达式匹配目标字符串，匹配成功后替换！
```

集合操作

```mysql
1） size： 集合中元素的个数
2） map_keys： 返回map中的key
3） map_values: 返回map中的value
4） array_contains: 判断array中是否包含某个元素
5） sort_array： 将array中的元素排序
```

### 系统内置函数

```mysql
1） 查看系统自带的函数
show functions;

2) 查询函数的用法
desc function extended 函数名
```

### 常用的内置函数

#### 空字段赋值 NVL 

```mysql
-- 语法：
nvl(value,default_value)

-- 说明：
1）如果value 为null，则返回default_value ，否则返回vaule；
2）如果两个值（value , default_value）均为null，则返回null；
```

#### CASE WHEN

```mysql
-- 示例：
select 
  dept_id,
  sum(case sex when '男' then 1 else 0 end) male_count,
  sum(case sex when '女' then 1 else 0 end) female_count
from 
  emp_sex
group by
  dept_id;
  
  /*  解读：
  1.按照dept_id 进行分组，同一组的数据先进行计算；
  2.假设dept_id=10的数据有10条，则10数据分别在sum函数中进行计算，计算完成以后得出一个结果；
  3.一组数据最后得到一条数据结果。
  */
```

#### 行转列

```mysql
-- 相关函数
1） concat('str1','str2','str3',...) : 表示将str1/str2/str3... 依次进行连接，str1/str2/str3... 可以说任何数据类型；
-- 示例：SELECT  concat('132','-','456'); ==> 132-456

2) concat_ws('连接符'，'str1','str2',...) : 表示使用'连接符'将str1/str2...依次进行连接，str1/str2...只能是字符串或者是字符串数组。
-- 示例：
SELECT  concat_ws('-','java','maven'); ==> java-maven;
SELECT  concat_ws(null,'java','maven'); ==> null -- 当连接符为null时，结果返回null
SELECT  concat_ws('.', 'www', array('facebook', 'com')) ；==> www.facebook.com

3） collect_set(col) : 函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段
-- 示例：
SELECT COLLECT_set(deptno) from emp; ==>[20,30,10]

```

#### 列转行

```mysql
-- 语法：
lateral view explode (split(字段，分割符)) 表名 as 列名
-- 说明：
lateral view : 侧写；
explode(): 将指定的集合拆解分成多行 -- 炸裂
split(字段，分割符) : 将指定的字符串按照分割符封装成一个集合。

-- 示例：
SELECT movie,category_name 
FROM movie_info 
lateral VIEW
explode(split(category,",")) movie_info_tmp  AS category_name ; -- categor_name 为炸裂的列名，move_info_tmp为侧写的表名

```

![image-20200630212310025](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200630212310.png)

![image-20200630212550900](https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200630212551.png)

### 开窗函数

```mysql
相关函数说明：开窗函数是为每一条数据进行开窗
1） over() : 单独使用此函数，默认的窗口大小为结果集的大小。
2） partition by : 在窗口函数中进行分区
  over(partition by 字段) ：对结果集内进行分区，每条数据的开窗大小为该结果集中分区集的大小。
3) over( order by 字段) ： 在窗口函数中只用到了order by 排序时，也会对每条数据进行开一个窗口，默认的开窗大小为：从结果集的开始位置到当前处理数据的位置。
-- 实例：
-- 1.查询在2017年4月份购买过的顾客及总人数
-- 解析，顾客全部要，多个顾客，多行，人数为一个值，一行，则是需要进行开窗，因为不是一一匹配的。
        SELECT  name ,
        COUNT(*)   OVER () `人数`
        from business 
        WHERE  SUBSTRING(orderdate,1,7)='2017-04' 
        group by name ;

-- 2.查询顾客的购买明细及月购买总额
    SELECT name ,orderdate ,cost ,
    sum (cost) over(partition by MONTH (orderdate))
    from business;
    
-- 3.上述的场景, 将每个顾客的cost按照日期进行累加
    SELECT name ,orderdate ,cost ,
    sum (cost) over(partition by name order by orderdate)
    from business;
    
4） CURRENT ROW：当前行
  n PRECEDING：往前n行数据
  n FOLLOWING：往后n行数据
5）UNBOUNDED：起点，
  UNBOUNDED PRECEDING 表示从前面的起点 
    UNBOUNDED FOLLOWING 表示到后面的终点
6）LAG(col,n,default_val)：往前第n行数据
7）LEAD(col,n, default_val)：往后第n行数据
8）NTILE(n)：把有序窗口的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回此行所属的组的编号。注意：n必须为int类型。
示例：
-- 需求：查询前20%时间的订单信息
select * from (
    select name,orderdate,cost, ntile(5) over(order by orderdate) sorted
    from business
) t
where sorted = 1;
```

### Rank

```mysql
-- 函数说明
1) RANK() 排序相同时会重复，总数不会变; 
   -- 1 2 2 4 5 5 7
2) DENSE_RANK() 排序相同时会重复，总数会减少; 
  -- 1 2 2 3 3 4 4 5
3) ROW_NUMBER() 会根据顺序计算。
  -- 1 2 3 4 5 6 
```

### 自定义函数

```mysql
自定函数的分类：
1） UDF（User-Defined-Function） -- 一进一出

2） UDAF（User-Defined Aggregation Function） -- 聚集函数，多进一出
  类似于：count/max/min
  
3） UDTF（User-Defined Table-Generating Functions） -- 一进多出
  如lateral view explode()
```

#### 自定义UDF函数

1. 需求：UDF实现计算给定字符串的长度

```mysql
示例：
select my_len("abcd"); ==> 4 
```

2. 创建一个Maven工程
3. 导入依赖

```java
<dependencies>
    <dependency>
      <groupId>org.apache.hive</groupId>
      <artifactId>hive-exec</artifactId>
      <version>3.1.2</version>
    </dependency>
</dependencies>
```

4. 创建一个类继承于GenericUDF

```java
package com.lianzp.hive;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;
import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

/**
 * 自定义UDF函数，需要继承GenericUDF类
 * 需求: 计算指定字符串的长度
 */
public class MyStringLength extends GenericUDF {
    /**
     *
     * @param arguments 输入参数类型的鉴别器对象
     * @return 返回值类型的鉴别器对象
     * @throws UDFArgumentException
     */
    @Override
    public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {
        // 判断输入参数的个数
        if(arguments.length !=1){
            throw new UDFArgumentLengthException("Input Args Length Error!!!");
        }
        // 判断输入参数的类型
        if(!arguments[0].getCategory().equals(ObjectInspector.Category.PRIMITIVE)){
            throw new UDFArgumentTypeException(0,"Input Args Type Error!!!");
        }
        //函数本身返回值为int，需要返回int类型的鉴别器对象
        return PrimitiveObjectInspectorFactory.javaIntObjectInspector;
    }

    /**
     * 函数的逻辑处理
     * @param arguments 输入的参数
     * @return 返回值
     * @throws HiveException
     */
    @Override
    public Object evaluate(DeferredObject[] arguments) throws HiveException {
       if(arguments[0].get() == null){
           return 0 ;
       }
       return arguments[0].get().toString().length();
    }

    @Override
    public String getDisplayString(String[] children) {
        return "";
    }
}
```

5. 打成jar包上传到服务器/opt/module/hive/datas/myudf.jar
6. 将jar包添加到hive的classpath

```mysql
add jar /opt/module/hive/datas/myudf.jar;
```

7. 创建临时函数与开发好的java class关联

```mysql
create temporary function my_len as "com.lianzp.hive. MyStringLength";
```

8. 即可在hql中使用自定义的函数my_len

```mysql
select ename,my_len(ename) ename_len from emp;
```

#### 自定义UDTF函数

和udf的最大区别就是自定义函数不同。

```java
package com.lianzp.udtf;

import org.apache.hadoop.hive.ql.exec.UDFArgumentException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;
import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;

import java.util.ArrayList;
import java.util.List;

public class MyUDTF extends GenericUDTF {

    private ArrayList<String> outList = new ArrayList<>();

    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {


        //1.定义输出数据的列名和类型
        List<String> fieldNames = new ArrayList<>();
        List<ObjectInspector> fieldOIs = new ArrayList<>();

        //2.添加输出数据的列名和类型
        fieldNames.add("lineToWord");
        fieldOIs.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);

        return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);
    }

    @Override
    public void process(Object[] args) throws HiveException {
        
        //1.获取原始数据
        String arg = args[0].toString();

        //2.获取数据传入的第二个参数，此处为分隔符
        String splitKey = args[1].toString();

        //3.将原始数据按照传入的分隔符进行切分
        String[] fields = arg.split(splitKey);

        //4.遍历切分后的结果，并写出
        for (String field : fields) {

            //集合为复用的，首先清空集合
            outList.clear();

            //将每一个单词添加至集合
            outList.add(field);

            //将集合内容写出
            forward(outList);
        }
    }

    @Override
    public void close() throws HiveException {

    }
}
```



## hive优化

### 建表优化

#### 分区表

建立分区表

```sql
hive (default)> load data local inpath '/opt/module/data/dept_20200401.log' into table dept_partition partition(day='20200401');
```



粒度不够可以建立二级分区

```sql
hive (default)> 
create table dept_partition2( deptno int,
dname string, loc string)
partitioned by (day string, hour string)
row format delimited fields terminated by '\t';
```



动态分区

```sql
--开启动态分区 
set hive.exec.dynamic.partition=true;
```



设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区

```sql
set hive.exec.dynamic.partition.mode=nonstrict;
```



```sql
insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept;
```



#### 分桶表

分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。

分桶是将数据集分解成更容易管理的若干部分的另一个技术。分区针对的是数据的存储路径，分桶针对的是数据文件

```sql
create table stu_buck(id int, name string)
clustered by(id) 
into 4 buckets
row format delimited fields terminated by '\t';
```



根据hash值取余放到不同的文件中



抽样查询

```sql
hive (default)> select * from stu_buck tablesample(bucket 1 out of 4 on id);
```



注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。

y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。

x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。



行存储特点：

查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。

列存储特点：

因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。

TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的

ORC 和PARQUET 是基于列式存储的



![1662544459200](https://gitee.com/it-wont-work/typora-cloud-map-library/raw/master/img/1662544459200.png)



### HQL语法

#### 列裁剪和分区裁剪

列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低



#### group by 

数据倾斜

```sql
是否在 Map 端进行聚合，默认为 True //预聚合
set hive.map.aggr = true;
在 Map 端进行聚合操作的条目数目
set hive.groupby.mapaggr.checkinterval = 100000;
有数据倾斜的时候进行负载均衡（默认是 false）
set hive.groupby.skewindata = true; //先打散，再聚合
```



#### 多重模式

```sql
insert int t_ptn partition(city=A). select id,name,sex, age from student where city= A;
insert int t_ptn partition(city=B). select id,name,sex, age from student where city= B;
insert int t_ptn partition(city=c). select id,name,sex, age from student where city= c;

修改为：
from student
insert int t_ptn partition(city=A) select id,name,sex, age where city= A insert int t_ptn partition(city=B) select id,name,sex, age where city= B
```



#### in/exists

select a.id, a.name from a where a.id in (select b.id from b);

select a.id, a.name from a where exists (select id from b where a.id = b.id);



select a.id, a.name from a left semi join b on a.id = b.id;



#### CBO

优化默认开启

CBO，成本优化器，代价最小的执行计划就是最好的执行计划

优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等



#### 谓词下推

where尽可能提前执行



#### map join

MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，**这样就不用进行** **Reduce 步骤**，从而提高了速度。如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map 端进行 Join，避免 Reducer 处理



使用left Join时会失效

（1） 设置自动选择 MapJoin

set hive.auto.convert.join=true; #默认为 true  

（2） 大表小表的阈值设置（默认 25M 以下认为是小表）：

set hive.mapjoin.smalltable.filesize=25000000;



#### SMB join

SMB Join ：Sort Merge Bucket Join 

 将两个表做成分桶表，切分成多个小文件

拆开成小表

分桶数和第一张表的分桶数为倍数关系



### 数据倾斜

group by

数据倾斜

```sql
是否在 Map 端进行聚合，默认为 True //预聚合
set hive.map.aggr = true;
在 Map 端进行聚合操作的条目数目
set hive.groupby.mapaggr.checkinterval = 100000;
有数据倾斜的时候进行负载均衡（默认是 false）
set hive.groupby.skewindata = true; //先打散，再聚合
当选项设定为 true，生成的查询计划会有两个 MR Job
```



增加reducer的个数（多个key同时导致数据倾斜）

在 hadoop 的 mapred-default.xml 文件中修改设置每个 job 的 Reduce 个数

set mapreduce.job.reduces = 15;



通过抽样看是多个key导致还是单个key导致



```sql
#join 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置
set hive.skewjoin.key=100000;
# 如果是 join 过程出现倾斜应该设置为 true 
set hive.optimize.skewjoin=false;
```



### 查看执行计划

stage间的依赖关系

每个stage所包含的operator



# Spark

## Spark WordCount

```scala
object WordCount {

  def main(args: Array[String]): Unit = {


    //1、创建SparkContext
    //   master: local/local[N]/local[*]
    //  提交代码到集群执行的时候,setMaster不需要设置，后续通过spark-submit --master 指定
    val conf = new SparkConf().setMaster("local[4]").setAppName("wordcount")
    val sc = new SparkContext(conf)
    //2、读取文件
    val rdd1: RDD[String] = sc.textFile("datas/wc.txt")
    //3、切分、压平
    //val rdd2 = rdd1.flatMap(line=>line.split(" "))
    val rdd2 = rdd1.flatMap(_.split(" "))

    //4、分组+聚合
    //val rdd3 = rdd2.groupBy(x=>x)

    //val rdd4 = rdd3.map(x=>(x._1,x._2.size))
    val rdd3 = rdd2.map(x=>(x,1))

    val rdd4 = rdd3.reduceByKey((agg,curr)=> agg+curr)

    //5、结果展示
    val result = rdd4.collect()
    println(result.toList)

    sc.stop()
  }
}
```



## Spark算子

### transformation算子

| map          | 映射                 |
| ------------ | -------------------- |
| mapPartitons | 以分区为单位进行映射 |
| flatMap      | 扁平化               |
| groupBy      | 分组                 |
| filter       | 过滤                 |
| distinct     | 去重                 |
| repartiton   | 重新分区             |
| sortBy       | 排序                 |

### action算子

| reduce      | 聚合                             |
| ----------- | -------------------------------- |
| collect     | 以数组形式返回数据集             |
| count       | 返回RDD中元素个数                |
| foreach     | 遍历RDD的每一个元素              |
| take        | 返回前n个元素组成的数组          |
| takeOrdered | 返回RDD排序后前n个元素组成的数组 |



### reducebykey 和groupbykey区别

性能：

都存在shuffle操作

reducebykey在分区里进行预聚合操作，减少shuffle时落盘的数据量，提高shuffle性能，减少磁盘IO

功能：

reducebykey包含分组和聚合功能，groupbykey只进行分组，分组聚合场景下，使用reducebykey,只是分组不聚合，使用groupbykey



### reduceBykey、aggregateByKey区别

aggregateByKey可用于计算平均值

| ReduceByKey    | 没有初始值           | 分区内和分区间逻辑相同                                       |
| -------------- | -------------------- | ------------------------------------------------------------ |
| foldByKey      | 有初始值             | 分区内和分区间逻辑相同时代替aggregateByKey**简化**代码，只写一个逻辑 |
| aggregateByKey | 有初始值             | 分区内和分区间逻辑可以不同                                   |
| combineByKey   | 初始可以**变换结构** | 分区内和分区间逻辑不同                                       |

```
// combineByKey : 方法需要三个参数
// 第一个参数表示：将相同key的第一个数据进行结构的转换，实现操作
// 第二个参数表示：分区内的计算规则
// 第三个参数表示：分区间的计算规则
```



## Spark优化

### 资源调优

#### Spark提交参数

以128G内存，32线程为例

分配给spark 28线程 内存100G

| executor-cores  | executor 的最大核数                        | 3-6   一般设置为4 |
| --------------- | ------------------------------------------ | ----------------- |
| num-executors   | 每个节点的executor 数* work 节点数(机器数) | (28/4)* 7 = 49G   |
| executor-memory | yarn内存数/每个节点的executor数            | 100G/7 = 14G      |



#### Spark内存分布

| Other 内存    | 自定义数据结构*每个 Executor 核数         | 默认占可用内存40% |
| ------------- | ----------------------------------------- | ----------------- |
| Storage 内存  | 广播变量+ cache/Executor 数量             | 默认占统一内存50% |
| Executor 内存 | 每个 Executor 核数* （数据集大小/并行度） | 默认占统一内存50% |
| 统一内存      | Storage 内存+Executor 内存                | 默认占可用内存60% |



Executor示例：4 * （100G/200) = 2G



#### Spark 内存优化

rdd缓存采用kryo序列化，大幅减少资源占用

df、ds设置存储级别为 MEMORY_AND_DISK_SER（设不设置区别不大）

使用默认级别MEMORY_ONLY 是对 CPU 的支持最好的。但是序列化缓存可以让体积更小，那么当 yarn 内存资源不充足情况下可以考虑使用 MEMORY_ONLY_SER 配合 kryo 使用序列化缓存



#### CPU优化

一般会将并行度（task 数）设置成并发度

（vcore 数）的 2 倍到 3 倍

并发度 = num-executors * executor-cores



### SparkSQL语法优化

SparkSQL 在整个执行计划处理的过程中，使用了 Catalyst 优化器

#### RBO优化

##### 谓词下推

看执行计划，优化后的逻辑计划能先过滤的就先过滤

先filter再join，inner join对两张表都进行了过滤

##### 列裁剪

列剪裁就是扫描数据源的时候，只读取那些与查询相关的字段

##### 常量替换

例如：用5代替2+3



#### CBO优化 

先对表、列进行数据的统计

开启cbo后可能会自动广播小表



#### 广播小表

默认小于10M自动广播

强制广播

```sql
    val sqlstr1 =
      """
        |select /*+  BROADCASTJOIN(sc) */
        |  sc.courseid,
        |  csc.courseid
        |from sale_course sc join course_shopping_cart csc
        |on sc.courseid=csc.courseid
      """.stripMargin
```



#### SMB JOIN

SMB JOIN 是 sort merge bucket 操作，需要进行分桶，首先会进行排序，然后根据 key 值合并，把相同 key  的数据放到同一个 bucket  中（按照 key  进行 hash）。分桶的目的其实就是把大表化成小表。相同 key 的数据都在同一个桶中之后，再进行 join 操作，那么在联合的时候就会大幅度的减小无关项的扫描。

条件：

（1） 两表进行分桶，桶的个数必须相等

（2）两边进行 join 时，join 列=排序列=分桶列



## Spark数据倾斜

| 使用场景                                                     | 解决方案                                                     | 优点                            | 缺点                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------- | --------------------------------------------------------- |
| Hive表本身数据就不均匀，而且业务场景需要频繁去用spark对Hive表执行分析操作 | 可以提前在Hive中就做聚合或者join操作                         | 简单便捷、完全规避掉shuffle阶段 | 治标不治本，HiveETL中还是会发生数据倾斜                   |
| 对RDD执行聚合类shuffle算子或者在SparkSql中使用group by进行分组聚合时 | 第一次局部聚合，先给key打上随机数前缀，接着执行聚合操作，将随机数前缀去掉，再次进行聚合操作 | 对于聚合类shuffle操作效果是很好 | 适用范围比较窄，如果是join类的shuffle操作就不行           |
| 大小表join                                                   | 广播小表                                                     | 简单便捷、完全规避掉shuffle阶段 | 适用场景小，只适合大小表join，而且会比较消耗内存，容易OOM |
| 两个大表join，一张倾斜key的数量比较多，另一张key分布均匀     | 将正常表key扩容N倍，对倾斜表中数据每一个都打上N以内的随机数，然后join | 对Join类型的数据基本都可以处理  | 只是缓解数据倾斜，而且对内存资源要求很高                  |



# Flink

## 调优

### 资源配置调优 

- jobmanager内存：2G
- taskmanager slot 数： container中cpu和slot的比例： 1：1    2个CPU 2个核
- taskmanager：1个CPU4G内存，所以8G



```shell
bin/flink run \
-t yarn-per-job \
-d \
-p 5 \ 指定并行度
-Dyarn.application.queue=test \ 指定 yarn 队列
-Djobmanager.memory.process.size=2048mb \ JM2~4G 足够
-Dtaskmanager.memory.process.size=4096mb \ 单个 TM2~8G 足够
-Dtaskmanager.numberOfTaskSlots=2 \ 与容器核数 1core：1slot 或 2core：1slot
-c com.atguigu.flink.tuning.UvDemo \
/opt/module/flink-1.13.1/myjar/flink-tuning-1.0-SNAPSHOT.jar
```



source的数据源端是kafka,Source的并行度设置为Kafka对应的topic分区数



RocksDB  磁盘+内存的模式 能开启增量检查点



### 反压

![1663088811476](https://gitee.com/it-wont-work/typora-cloud-map-library/raw/master/img/1663088811476.png)



定位：下游为绿，自己红的算子



三种原因和解决方案

资源不足：提高并行度、内存、加机器

数据倾斜：只有个别Backpressure status为红,

和第三方数据库交互：使用旁路缓存和异步IO解决





### 数据倾斜

现象：相同 Task 的多个 Subtask 中， 个别 Subtask 接收到的数据量明显大于其他Subtask 接收到的数据量，通过 Flink Web UI 可以精确地看到每个 Subtask 处理了多少数据，即可判断出 Flink 任务是否存在数据倾斜。通常，数据倾斜也会引起反压

keyby之前数据倾斜：rebalance 重分区全局轮询，尽量在上游处理

keyby之后数据倾斜

- keyby后直接聚合（来一条处理一条）

  - 两阶段聚合会导致数据重复计算，不准确，不采用
  - 预聚合：定时器+状态(普通的算子状态)，采用，开窗聚合不行：会使用keyby

- keyby后开窗聚合(一个窗口输出一条)

  - 加随机数实现两阶段聚合，采用，
    - 第一阶段key拼接随机数前缀，进行keyby开窗
    - 第二阶段聚合：去掉随机数前缀，加上windowend(窗口结束时间)作为key

  ![1663087797387](https://gitee.com/it-wont-work/typora-cloud-map-library/raw/master/img/1663087797387.png)

  - 预聚合：会丢失原始的数据时间，时间语义就没了



# 离线数仓



# 实时数仓