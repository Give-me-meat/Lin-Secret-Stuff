# 面试题准备

## 项目介绍

离线数仓：当时我们从0到1搭建了离线数仓。选的是Hive on Spark架构，使用spark作为计算引擎。采集数据的话，我们当时日志数据是flume采集数据到kafka，然后再通过flume传输到hdfs上。业务数据就是从mysql通过sqoop传到hdfs上。在hdfs上我们对数据进行分层，分为ods+dwd+dws+ads层。ods层是存放原始数据，dwd层对数据进行清洗，去掉过期、重复、不完整的数据。dws对数据汇总。ads层为统计报表提供数据。最后数据还是通过sqoop导出到mysql。



实时数仓：接下来是实时数仓，日志数据通过flume采集到kafka,业务数据通过maxwell读取mysql的binlog文件采集到kafka，形成ods层.之后对数据进行分层，分为dim层、dwd层、dws层、ads层，dim层的数据存入hbase,dwd层是存入kafka的不同主题，dws层写入clickhouse，ads层调用接口对clickhouse进行查询。



小胖熊数据决策平台：职责是服务中台，和业务以及一些业务部门的数据分析人员对接。会维护BI平台的日常运转以及看板和报表的开发，主要是通过FineBI和FineReporter实现。



## java和Scala的区别

| 区别                   | java                     | scala                           |
| ---------------------- | ------------------------ | ------------------------------- |
| 方法返回值             | 需要显示的return进行返回 | sacla可选，他是默认返回最后一行 |
| 接口                   | java支持接口             | 不支持接口，采用trait           |
| 类和方法修饰符的默认值 | protected                | public                          |
| 样例类                 |                          | 自动生成toString,equals方法     |



## linux常用命令

| 常用命令或参数         | 功能                                         | 额外说明                                                     |
| ---------------------- | -------------------------------------------- | ------------------------------------------------------------ |
| nohup                  | 免疫关闭session所发送的SIGHUP信号            | 常与&配合使用                                                |
| &                      | 免疫Ctrl + C发送的SIGINT信号                 | 常与nohup配合使用                                            |
| >/dev/null             | 将标准输出1重定向到`/dev/null`中             | /dev/null代表[linux](https://so.csdn.net/so/search?from=pc_blog_highlight&q=linux)的空设备文件，所有往这个文件里面写入的内容都会丢失，俗称“黑洞” |
| 2>&1                   | 重定向绑定                                   | 错误输出将会和标准输出输出到同一个地方                       |
| ps -ef/aux \|grep 进程 | 查看进程端口号                               |                                                              |
| grep -v                | 过滤不需要的进程                             | 其他参数：-i 忽略大小写 -E支持正则表达式                     |
| \|                     | 上一条命令的标准输出，作为下一条命令标准输入 | 示例：echo "--help" \| cat                                   |
| xargs                  | 上一条命令的标准输出，作为下一条命令的参数   | 示例：echo "--help" \| xargs cat                             |
| awk                    | 文本分析工具                                 | 示例：awk '{print $2 }' 返回分隔符的第二位，默认分隔符为空格，指定使用参数-F |
| ls -a                  | 显示隐藏文件                                 | 可显示后缀为.swp的文件                                       |
| rpm -qa                | 显示所有rpm的安装包                          |                                                              |
| rpm -e –nodeps         | 强制卸载安装包                               |                                                              |
| rpm -ivh               | 安装安装包                                   |                                                              |
| :g/^s*$/d              | 删除空行                                     | g ：全区命令 / ：分隔符 ^s*$ ：匹配空行，其中^表示行首，s表示空字符，包括空格和制表符，*重复0到n个前面的字符，$表示行尾。连起来就是匹配只有空字符的行，也就是空行。 /d ：删除该行 |
| %s/^\s*//g             | 删除行首空格                                 |                                                              |



## HDFS架构

NameNode：类似于Master

- 管理HDFS的名称空间
- 配置副本策略
- 管理数据块（Block)映射信息
- 处理客户端读写请求



DateNode:类似于Slave NameNode下达命令，DataNode执行实际的操作

- 存储实际的数据块
- 执行数据块的读写操作



Client:客户端

- 文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行上传
- 与NameNode交互，获取文件的位置信息
- 与DataNode交互，读取或写入数据
- Client提供一些命令来管理HDFS，比如NameNode格式化
- Client可以通过一些命令来访问HDFS，比如HDFS增删改查的操作



Secondary NameNode

- 辅助NameNode,分担其工作量
- 紧急情况下，辅助恢复NameNode



数据块大小：128M

1. 数据块设置太小，增加寻址时间（程序寻找块的开始位置）
2. 块设置太大，磁盘传输数据的时间明显大于定位块位置时间，处理块数据时速度非常慢



## HDFS写流程

1. Client向NameNode请求上传文件，NameNode检查文件是否存在
2. NameNode返回是否可以上传
3. Client向NameNode请求上传第一个Block，请求返回DataNode
4. NameNode返回DataNode（dn1,dn2,dn3),表示这三个节点可以存储数据
5. Client向DataNode请求建立通道
6. DataNode逐级应答客户端
7. 客户端以Parket为单位，dn1每收到一个Parket就会传给dn2,dn2到dn3，**dn1每传一个packet就会放入一个应答队列等待应答**
8. 当第一个Block传输完成之后，客户端再次请求NameNode上传第二个Block(重复3-7)



## HDFS读流程

1. 客户端向NameNode请求下载文件，NameNode通过查询元数据，找到DataNode所在地址
2. 挑选DataNode（就近原则）服务器，请求读取数据
3. DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位做校验）
4. 客户端以Packet为单位接收，先在本地缓存，然后写入目标文件



## Shuffle

Shuffle机制过程

1. maptask将map()方法输出的k,v对存放到环形缓冲区
2. 当环形缓冲区达到溢写比例时，一般为80%，从内存溢写文件到磁盘
3. 用快速排序对缓冲区里的数据进行排序，先按照分区编号Partiton排序，然后按照key排序
4. 将所有的溢出文件通过归并排序合并成一个大的溢出文件
5. reduceTask根据分区号从各个Maptask上取相应的数据
6. reduce会取到同一个分区来自不同MapTask的结果文件，并通过归并排序对这些文件再次进行排序
7. 生成大文件之后，shuffe结束，进入reduceTask的逻辑运算过程



## Yarn架构

ResouceManager

- 处理客户端请求
- 监控NodeManager
- 启动或监控ApplicationMaster
- 资源的分配与调度



NodeManager

- 管理单个节点上的资源
- 处理来自ResourManager的命令
- 处理来自ApplicationMaster的命令



ApplicationMaster

- 负责数据的切分(根据提交到HDFS的切片信息划分)
- 为应用程序申请资源并分配给内部的任务
- 任务的监控与容错



Container

- Container是Yarn中资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等



## Yarn工作机制

作业提交过程

1. Client向整个集群提交MapReduce作业
2. Client向RM申请一个作业id
3. RM给Client返回该job资源提交的路径和作业id
4. Client提交jar包、切片信息和配置文件到指定的资源提交路径
5. Client提交完资源后，向RM申请运行MrAppMaster



作业初始化

1. 当RM收到Client请求后，将job添加到容量调度器中
2. 某一个空闲的NM领取到Job
3. 该NM创建Container，并产生MRAppmaster
4. 下载Clinet提交的资源到本地



任务分配

1. MRAppMaster向RM申请运行多个MapTask任务资源
2. RM将运行MapTask任务分配给另外两个NodeManager,另两个NodeManager分别领取任务并创建容器



任务运行

1. MRAppMaster向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManger分别启动MapTask,MapTask对数据分区排序
2. MRAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask
3. ReduceTask向MapTask获取相应分区的数据
4. 程序运行完毕，MR会向RM申请注销自己



## flume三大组件

### Source

source组件用来收集数据的，可以收集各种类型和格式的日志数据

​	包括avro,thrift,taildirsource

​	taildirsource优点：支持断点续传、多目录

### Channel

可以对数据进行缓存，可以存在memory或者file中

| memory channel | 基于内存        | 效率高、可靠性低 |
| -------------- | --------------- | ---------------- |
| file channel   | 基于磁盘        | 效率低、可靠性高 |
| kafka channel  | 基于kafka的磁盘 | 效率高、可靠性高 |

选用memory channel

### Sink

用于把数据发送到目的地的组件

​	包括Hdfs、Logger、avro、thrift、Hbase





## flume拦截器

### JSON格式拦截器

阶段：flume至kafka,source至channel

作用：进行ETL清洗，过滤JSON格式不完整的数据

代码

```
package com.xpx.flume.interceptor;

import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.Iterator;
import java.util.List;

public class ETLInterceptor implements Interceptor {

    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {

        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);

        if (JSONUtils.isJSONValidate(log)) {
            return event;
        } else {
            return null;
        }
    }

    @Override
    public List<Event> intercept(List<Event> list) {

        Iterator<Event> iterator = list.iterator();

        while (iterator.hasNext()){
            Event next = iterator.next();
            if(intercept(next)==null){
                iterator.remove();
            }
        }

        return list;
    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new ETLInterceptor();
        }
        @Override
        public void configure(Context context) {

        }

    }

    @Override
    public void close() {

    }
}
```



### 时间戳拦截器

阶段：flume至hdfs

作用：解决零点漂移的问题，避免flume上传到hdfs时采用linux默认系统时间产生误差，需要取到日志生成的时间

解决方案：将event的body信息转化为json,获取其中时间戳的值，放入event的headers信息中，headers名字为timestamp

代码

### 自定义拦截器步骤

1. 实现Interceptor接口，重写4个方法
2. intercept方法处理单个event
3. Intercept方法处理event集合
4. 将event的body信息转化为json,获取其中时间戳的值，放入event的headers信息中，headers名字为timestamp
5. 增加静态内部类返回一个拦截器对象

代码

```java
package com.xpx.flume.interceptor;


import com.alibaba.fastjson.JSONObject;
import org.apache.flume.Context;
import org.apache.flume.Event;
import org.apache.flume.interceptor.Interceptor;

import java.nio.charset.StandardCharsets;
import java.util.List;
import java.util.Map;

public class TimestampInterceptor implements Interceptor {
    @Override
    public void initialize() {

    }

    @Override
    public Event intercept(Event event) {
        Map<String, String> headers = event.getHeaders();
        byte[] body = event.getBody();
        String log = new String(body, StandardCharsets.UTF_8);
        JSONObject jsonObject = JSONObject.parseObject(log);
        String ts = jsonObject.getString("ts");
        headers.put("timestamp",ts);
        return event;
    }

    @Override
    public List<Event> intercept(List<Event> list) {
        for (Event event : list) {
            intercept(event);
        }
        return list;
    }

    @Override
    public void close() {

    }

    public static class Builder implements Interceptor.Builder{

        @Override
        public Interceptor build() {
            return new TimestampInterceptor();
        }

        @Override
        public void configure(Context context) {

        }
    }
}

```



## hdfs小文件问题

小文件的危害

- 存储：占用namenode内存，每个小文件会占用150个字节
- 计算：默认切片规则为一个文件切一次，一个小文件对应一个maptask,一个maptask占用一个内存



①在flume的配置文件中配置参数

| 参数              | 解释                                                         |
| ----------------- | ------------------------------------------------------------ |
| hdfs.rollInterval | 文件创建超xxx秒时会滚动生成新文件                            |
| hdfs.rollSize     | 文件超过xxxM时会滚动生成新文件                               |
| hdfs.rollCount    | event个数达到xxx时会滚动生成新的文件（设置为0则永远不根据事件的数量滚动） |



vim kafka-flume-hdfs.conf

```shell
## 组件
a1.sources=r1
a1.channels=c1
a1.sinks=k1

## source1
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.batchSize = 5000
a1.sources.r1.batchDurationMillis = 2000
a1.sources.r1.kafka.bootstrap.servers = xpx101:9092,xpx102:9092,xpx103:9092
a1.sources.r1.kafka.topics=haha
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.xpx.flume.interceptor.TimeStampInterceptor$Builder

## channel1
a1.channels.c1.type = file
#磁盘数据在内存中索引落盘的位置
a1.channels.c1.checkpointDir = /opt/xpx/module/flume/checkpoint/behavior1
#数据备份的磁盘位置
a1.channels.c1.dataDirs = /opt/xpx/module/flume/data/behavior1/


## sink1
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = /origin_data/gmall/log/topic_log/%Y-%m-%d
#前缀
a1.sinks.k1.hdfs.filePrefix = log-
a1.sinks.k1.hdfs.round = false

#控制生成的小文件
a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

## 控制输出文件是原生文件。
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = lzop

## 拼装
a1.sources.r1.channels = c1
a1.sinks.k1.channel= c1

```



②设置 set hive.input.format 

hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;（在map端合并小文件）

但是在有LZO索引文件时会导致LZO索引文件丢失，此时需要取消合并小文件

set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;





## kafka

kafka的优势

消息队列：解耦、缓冲、削峰、灵活性高、异步

基于发布/订阅模式 相比点对点（一个消息只能被一个消费者消费）处理的更快

发布/订阅模式也分为队列推数据和消费者拉取数据

队列推数据：可能产生推数据过快，消费者来不及消费的情况，崩掉的情况，也有可能存在资源浪费的情况

消费者主动拉取缺点：consumer不断向topic进行轮询，造成资源的浪费



同一个消费者组的不同消费者不能够消费同一个主题的同一个分区



zookeeper作用：存储kafka的集群信息，存储kafka消费者的消费位置信息 0.9版本之后记录在系统主题当中，不存在zookeeper是因为consumer在消费的同时，和集群交互同时还要和zookeeper交互，效率太低

kafka消息存储在磁盘，默认存7天

### kafka broker

分区的好处：

提高负载

增加并行度

### kafka生产者分区规则

指定分区

根据key的哈希值取余分区数

round-robin轮询

kafka 常用命令

查看topic

```
 bin/kafka-topics.sh  --zookeeper xpx101:2181/kafka --list
```

创建topic

```
bin/kafka-topics.sh --zookeeper xpx101:2181/kafka --create --replication-factor 3 --partitions 3 --topic first
```

### ISR

ISR队列的作用：因为kafka副本同步策略选择的是所有的副本全部同步，但是可能存在部分机器损坏导致阻塞，通过设置最大等待时间的参数（默认10秒），超过这个时间就剔除ISR队列

follower被踢出ISR之后如何重新加入：将自身高于HW的部分截取掉，当自己的LEO到达该分区的HW后重新加入ISR

### ack级别

0：直接发  最容易丢数据

1：leader接收完  较可能丢数据

-1：leader和follower（ISR队列里的follower）都接收完  保证不丢数据，可能数据重复

### 保证数据一致性

LEO：每个副本的最后一个offset

HW：ISR副本中最小的LEO

消费端一致性：comsumer只能消费到HW

存储端一致性：leader挂掉时，新leader以外的ISR队列成员将HW以外的截取掉，统一向新leader同步

### Excatly Once 精准一次性消费

Excatly Once：ack=-1+幂等性（开启参数即可）

幂等性原理：在produce初始化时会生成pid，这个生产者的消息会附带序列化信息，broker端会有一个<pid,partition,seq>的缓存，一个信息只有持久化一次

因为produce重启会生成新的pid，所以不支持跨会话跨分区的精准一次

### 消费者分区分配规则

消费者数量变化时触发

RoundRobin:轮询  所有主题+分区作为整体，消费者之间资源最多只差一个，无法满足一个消费者组只消费一个主题的需求

Range:范围 每个主题作为一个整体，可以指定消费者组消费指定分区，但是可能消费者资源分配不均

默认range

### Offset维护

由主题+分区+消费者组确定一个offset

代码测试

创建主题

```
bin/kafka-topics.sh --create --topic bigdata --zookeeper xpx101:2181/kafka --partitions 2 --replication-factor 2
```

启动生产者

```
bin/kafka-console-producer.sh --broker-list xpx101:9092 --topic bigdata
```

启动消费者

```
bin/kafka-console-consumer.sh --bootstrap-server xpx101:9092 --topic bigdata --consumer-property group.id=test1-group --from-beginning
```

查看消费者的offset信息

```
./kafka-consumer-groups.sh --bootstrap-server xpx101:9092 --describe --group test1-group
```

zookeeper查看,新版kafka无法查看到controller节点下的信息

```
ls /kafka
get /kafka/controller
```

Kafka落盘仍高效的原因

分布式，有分区，并行度高

顺序写磁盘，减少寻址时间

使用了零拷贝技术，减少了传输过程，文件-操作系统-到用户层 简化为文件-操作系统-文件

### Controller作用

Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群broker的上下线，所有 topic 的分区副本分配和 leader选举

### kafka组件顺序

拦截器-序列化-分区器

### Producer事务

客户端提供唯一的事务ID（Transaction ID），将pid和事务ID绑定，重启之后根据Transaction ID获取pid,实现跨分区跨回话的精准一次

消费者事务则是消费一次



## Spark WordCount

```scala
object WordCount {

  def main(args: Array[String]): Unit = {


    //1、创建SparkContext
    //   master: local/local[N]/local[*]
    //  提交代码到集群执行的时候,setMaster不需要设置，后续通过spark-submit --master 指定
    val conf = new SparkConf().setMaster("local[4]").setAppName("wordcount")
    val sc = new SparkContext(conf)
    //2、读取文件
    val rdd1: RDD[String] = sc.textFile("datas/wc.txt")
    //3、切分、压平
    //val rdd2 = rdd1.flatMap(line=>line.split(" "))
    val rdd2 = rdd1.flatMap(_.split(" "))

    //4、分组+聚合
    //val rdd3 = rdd2.groupBy(x=>x)

    //val rdd4 = rdd3.map(x=>(x._1,x._2.size))
    val rdd3 = rdd2.map(x=>(x,1))

    val rdd4 = rdd3.reduceByKey((agg,curr)=> agg+curr)

    //5、结果展示
    val result = rdd4.collect()
    println(result.toList)

    sc.stop()
  }
}
```



## Spark算子

### transformation算子

| map          | 映射                 |
| ------------ | -------------------- |
| mapPartitons | 以分区为单位进行映射 |
| flatMap      | 扁平化               |
| groupBy      | 分组                 |
| filter       | 过滤                 |
| distinct     | 去重                 |
| repartiton   | 重新分区             |
| sortBy       | 排序                 |

### action算子

| reduce      | 聚合                             |
| ----------- | -------------------------------- |
| collect     | 以数组形式返回数据集             |
| count       | 返回RDD中元素个数                |
| foreach     | 遍历RDD的每一个元素              |
| take        | 返回前n个元素组成的数组          |
| takeOrdered | 返回RDD排序后前n个元素组成的数组 |



### reducebykey 和groupbykey区别

性能：

都存在shuffle操作

reducebykey在分区里进行预聚合操作，减少shuffle时落盘的数据量，提高shuffle性能，减少磁盘IO

功能：

reducebykey包含分组和聚合功能，groupbykey只进行分组，分组聚合场景下，使用reducebykey,只是分组不聚合，使用groupbykey



### reduceBykey、aggregateByKey区别

aggregateByKey可用于计算平均值

| ReduceByKey    | 没有初始值           | 分区内和分区间逻辑相同                                       |
| -------------- | -------------------- | ------------------------------------------------------------ |
| foldByKey      | 有初始值             | 分区内和分区间逻辑相同时代替aggregateByKey**简化**代码，只写一个逻辑 |
| aggregateByKey | 有初始值             | 分区内和分区间逻辑可以不同                                   |
| combineByKey   | 初始可以**变换结构** | 分区内和分区间逻辑不同                                       |

```
// combineByKey : 方法需要三个参数
// 第一个参数表示：将相同key的第一个数据进行结构的转换，实现操作
// 第二个参数表示：分区内的计算规则
// 第三个参数表示：分区间的计算规则
```



## Spark数据倾斜

| 使用场景                                                     | 解决方案                                                     | 优点                            | 缺点                                                      |
| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------- | --------------------------------------------------------- |
| Hive表本身数据就不均匀，而且业务场景需要频繁去用spark对Hive表执行分析操作 | 可以提前在Hive中就做聚合或者join操作                         | 简单便捷、完全规避掉shuffle阶段 | 治标不治本，HiveETL中还是会发生数据倾斜                   |
| 对RDD执行聚合类shuffle算子或者在SparkSql中使用group by进行分组聚合时 | 第一次局部聚合，先给key打上随机数前缀，接着执行聚合操作，将随机数前缀去掉，再次进行聚合操作 | 对于聚合类shuffle操作效果是很好 | 适用范围比较窄，如果是join类的shuffle操作就不行           |
| 大小表join                                                   | 广播小表                                                     | 简单便捷、完全规避掉shuffle阶段 | 适用场景小，只适合大小表join，而且会比较消耗内存，容易OOM |
| 两个大表join，一张倾斜key的数量比较多，另一张key分布均匀     | 将正常表key扩容N倍，对倾斜表中数据每一个都打上N以内的随机数，然后join | 对Join类型的数据基本都可以处理  | 只是缓解数据倾斜，而且对内存资源要求很高                  |



### 二次聚合

加随机数打散

```scala
object SkewAggregationTuning {
  def main( args: Array[String] ): Unit = {

    val sparkConf = new SparkConf().setAppName("SkewAggregationTuning")
      .set("spark.sql.shuffle.partitions", "36")
//      .setMaster("local[*]")
    val sparkSession: SparkSession = InitUtil.initSparkSession(sparkConf)

    sparkSession.udf.register("random_prefix", ( value: Int, num: Int ) => randomPrefixUDF(value, num))
    sparkSession.udf.register("remove_random_prefix", ( value: String ) => removeRandomPrefixUDF(value))


    val sql1 =
      """
        |select
        |  courseid,
        |  sum(course_sell) totalSell
        |from
        |  (
        |    select
        |      remove_random_prefix(random_courseid) courseid,
        |      course_sell
        |    from
        |      (
        |        select
        |          random_courseid,
        |          sum(sellmoney) course_sell
        |        from
        |          (
        |            select
        |              random_prefix(courseid, 6) random_courseid,
        |              sellmoney
        |            from
        |              sparktuning.course_shopping_cart
        |          ) t1
        |        group by random_courseid
        |      ) t2
        |  ) t3
        |group by
        |  courseid
      """.stripMargin


    val sql2=
      """
        |select
        |  courseid,
        |  sum(sellmoney)
        |from sparktuning.course_shopping_cart
        |group by courseid
      """.stripMargin

    sparkSession.sql(sql1).show(10000)


//    while(true){}
  }


  def randomPrefixUDF( value: Int, num: Int ): String = {
    new Random().nextInt(num).toString + "_" + value
  }

  def removeRandomPrefixUDF( value: String ): String = {
    value.toString.split("_")(1)
  }
}
```



## Spark提交参数

以128G内存，32线程为例

分配给spark 28线程 内存100G

| executor-cores  | executor 的最大核数                        | 3-6   一般设置为4 |
| --------------- | ------------------------------------------ | ----------------- |
| num-executors   | 每个节点的executor 数* work 节点数(机器数) | (28/4)* 7 = 49G   |
| executor-memory | yarn内存数/每个节点的executor数            | 100G/7 = 14G      |



## Spark内存分布

| Other 内存    | 自定义数据结构*每个 Executor 核数         | 默认占可用内存40% |
| ------------- | ----------------------------------------- | ----------------- |
| Storage 内存  | 广播变量+ cache/Executor 数量             | 默认占统一内存50% |
| Executor 内存 | 每个 Executor 核数* （数据集大小/并行度） | 默认占统一内存50% |
| 统一内存      | Storage 内存+Executor 内存                | 默认占可用内存60% |



Executor示例：4 * （100G/200) = 2G



## Spark 资源调优

### 内存优化

rdd缓存采用kryo序列化，大幅减少资源占用

df、ds设置存储级别为 MEMORY_AND_DISK_SER（设不设置区别不大）

使用默认级别MEMORY_ONLY 是对 CPU 的支持最好的。但是序列化缓存可以让体积更小，那么当 yarn 内存资源不充足情况下可以考虑使用 MEMORY_ONLY_SER 配合 kryo 使用序列化缓存



### CPU优化

一般会将并行度（task 数）设置成并发度

（vcore 数）的 2 倍到 3 倍

并发度 = num-executors * executor-cores



## SparkSQL语法优化

SparkSQL 在整个执行计划处理的过程中，使用了 Catalyst 优化器

### RBO优化

#### 谓词下推

看执行计划，优化后的逻辑计划能先过滤的就先过滤

先filter再join，inner join对两张表都进行了过滤

#### 列裁剪

列剪裁就是扫描数据源的时候，只读取那些与查询相关的字段

#### 常量替换

例如：用5代替2+3



### CBO优化 

先对表、列进行数据的统计

开启cbo后可能会自动广播小表



## 广播小表

默认小于10M自动广播

强制广播

```sql
    val sqlstr1 =
      """
        |select /*+  BROADCASTJOIN(sc) */
        |  sc.courseid,
        |  csc.courseid
        |from sale_course sc join course_shopping_cart csc
        |on sc.courseid=csc.courseid
      """.stripMargin
```



## SMB JOIN

SMB JOIN 是 sort merge bucket 操作，需要进行分桶，首先会进行排序，然后根据 key 值合并，把相同 key  的数据放到同一个 bucket  中（按照 key  进行 hash）。分桶的目的其实就是把大表化成小表。相同 key 的数据都在同一个桶中之后，再进行 join 操作，那么在联合的时候就会大幅度的减小无关项的扫描。

条件：

（1） 两表进行分桶，桶的个数必须相等

（2）两边进行 join 时，join 列=排序列=分桶列



## Typora快捷键

有序列表：1.+TAB

无序列表：*+TAB



## 面试问题

项目中遇到的问题：hdfs小文件问题

a1.sinks.k1.hdfs.rollInterval = 10
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0

设置参数控制



kafka数据重复：设置ack级别设置为—1



spark中



项目中的优化：redis旁路缓存和异步io



## sql执行计划

最重要字段：id、type、key、rows、extra

### id

id:select查询的序列号，包含一组数字，表示查询中执行select子句或操作表的顺序 

id相同：执行顺序由上至下

id不同：如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行



### type

访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是

system > const > eq_ref > ref > fulltext > ref_or_null > index_merge > unique_subquery > index_subquery > range > index > ALL



all:遍历全表找到匹配的行

index:索引全表扫描

range:表示范围索引扫描，常见于>，<，between，in，like，is null等运算的查询中

REF：非唯一性索引扫描，返回匹配某个单独值的所有行。本质是也是一种索引访问，它返回所有匹配某个单独值的行，然而他可能会找到多个符合条件的行，所以它应该属于查找和扫描的混合体

 

### key

实际使用的索引，如果为NULL，则没有使用索引。



### row

根据表统计信息及索引选用情况，大致估算出找到所需的记录所需要读取的行数



### Extra

不适合在其他字段中显示，但是十分重要的额外信息

Using temporary： 
使用临时表保存中间结果，也就是说mysql在对查询结果排序时使用了临时表，常见于order by 和 group by 

Using index： 
表示相应的select操作中使用了覆盖索引（Covering Index），避免了访问表的数据行，效率高 

Using where ： 
使用了where过滤



## sql优化

- **在表中建立索引**，优先考虑where、group by使用到的字段

- **尽量避免使用select ***，使用具体的字段代替

- **尽量避免使用in 和not in，会导致数据库引擎放弃索引进行全表扫描**。如下：

  SELECT * FROM t WHERE id IN (2,3)

  SELECT * FROM t1 WHERE username IN (SELECT username FROM t2)

  优化方式：如果是连续数值，可以用between代替。如下：

  SELECT * FROM t WHERE id BETWEEN 2 AND 3

  如果是子查询，可以用exists代替。如下：

  SELECT * FROM t1 WHERE EXISTS (SELECT * FROM t2 WHERE t1.username = t2.username)

- **尽量避免使用or，会导致数据库引擎放弃索引进行全表扫描。**如下：

  SELECT * FROM t WHERE id = 1 OR id = 3

  优化方式：可以用union代替or。如下：

  SELECT * FROM t WHERE id = 1

  UNION

  SELECT * FROM t WHERE id = 3

- **尽量避免在字段开头模糊查询**，会导致数据库引擎放弃索引进行全表扫描。如下：

  SELECT * FROM t WHERE username LIKE '%li%'

  优化方式：尽量在字段后面使用模糊查询。如下：

  SELECT * FROM t WHERE username LIKE 'li%'

- **尽量避免进行null值的判断**，会导致数据库引擎放弃索引进行全表扫描。如下：

  SELECT * FROM t WHERE score IS NULL

  优化方式：可以给字段添加默认值0，对0值进行判断。如下：

  SELECT * FROM t WHERE score = 0

- **尽量避免在where条件中等号的左侧进行表达式**、函数操作，会导致数据库引擎放弃索引进行全表扫描。如下：

  SELECT * FROM t2 WHERE score/10 = 9

  SELECT * FROM t2 WHERE SUBSTR(username,1,2) = 'li'

  优化方式：可以将表达式、函数操作移动到等号右侧。如下：

  SELECT * FROM t2 WHERE score = 10*9

  SELECT * FROM t2 WHERE username LIKE 'li%'

- 当数据量大时，避免使用where 1=1的条件。通常为了方便拼装查询条件，我们会默认使用该条件，数据库引擎会放弃索引进行全表扫描。如下：

  SELECT * FROM t WHERE 1=1

  优化方式：用代码拼装sql时进行判断，没where加where，有where加and



## hive优化

### 建表优化

#### 分区表

建立分区表

```sql
hive (default)> load data local inpath '/opt/module/data/dept_20200401.log' into table dept_partition partition(day='20200401');
```



粒度不够可以建立二级分区

```sql
hive (default)> 
create table dept_partition2( deptno int,
dname string, loc string)
partitioned by (day string, hour string)
row format delimited fields terminated by '\t';
```



动态分区

```sql
--开启动态分区 
set hive.exec.dynamic.partition=true;
```



设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区

```sql
set hive.exec.dynamic.partition.mode=nonstrict;
```



```sql
insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept;
```



#### 分桶表

分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区。对于一张表或者分区，Hive 可以进一步组织成桶，也就是更为细粒度的数据范围划分。

分桶是将数据集分解成更容易管理的若干部分的另一个技术。分区针对的是数据的存储路径，分桶针对的是数据文件

```sql
create table stu_buck(id int, name string)
clustered by(id) 
into 4 buckets
row format delimited fields terminated by '\t';
```



根据hash值取余放到不同的文件中



抽样查询

```sql
hive (default)> select * from stu_buck tablesample(bucket 1 out of 4 on id);
```



注：tablesample是抽样语句，语法：TABLESAMPLE(BUCKET x OUT OF y) 。

y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。

x表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第3(x+y)个bucket的数据。



行存储特点：

查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。

列存储特点：

因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。

TEXTFILE 和 SEQUENCEFILE 的存储格式都是基于行存储的

ORC 和PARQUET 是基于列式存储的



![1662544459200](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662544459200.png)



### HQL语法

#### 列裁剪和分区裁剪

列裁剪就是在查询时只读取需要的列，分区裁剪就是只读取需要的分区。当列很多或者数据量很大时，如果 select * 或者不指定分区，全列扫描和全表扫描效率都很低



#### group by 

数据倾斜

```sql
是否在 Map 端进行聚合，默认为 True //预聚合
set hive.map.aggr = true;
在 Map 端进行聚合操作的条目数目
set hive.groupby.mapaggr.checkinterval = 100000;
有数据倾斜的时候进行负载均衡（默认是 false）
set hive.groupby.skewindata = true; //先打散，再聚合
```



#### 多重模式

```sql
insert int t_ptn partition(city=A). select id,name,sex, age from student where city= A;
insert int t_ptn partition(city=B). select id,name,sex, age from student where city= B;
insert int t_ptn partition(city=c). select id,name,sex, age from student where city= c;

修改为：
from student
insert int t_ptn partition(city=A) select id,name,sex, age where city= A insert int t_ptn partition(city=B) select id,name,sex, age where city= B
```



#### in/exists

select a.id, a.name from a where a.id in (select b.id from b);

select a.id, a.name from a where exists (select id from b where a.id = b.id);



select a.id, a.name from a left semi join b on a.id = b.id;



#### CBO

优化默认开启

CBO，成本优化器，代价最小的执行计划就是最好的执行计划

优化每个查询的执行逻辑和物理执行计划。这些优化工作是交给底层来完成的。根据查询成本执行进一步的优化，从而产生潜在的不同决策：如何排序连接，执行哪种类型的连接，并行度等等



#### 谓词下推

where尽可能提前执行



#### map join

MapJoin 是将 Join 双方比较小的表直接分发到各个 Map 进程的内存中，在 Map 进程中进行 Join 操 作，**这样就不用进行** **Reduce 步骤**，从而提高了速度。如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在Reduce 阶段完成 Join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 Map 端进行 Join，避免 Reducer 处理



使用left Join时会失效



（1） 设置自动选择 MapJoin

set hive.auto.convert.join=true; #默认为 true	

（2） 大表小表的阈值设置（默认 25M 以下认为是小表）：

set hive.mapjoin.smalltable.filesize=25000000;



#### SMB join

SMB Join ：Sort Merge Bucket Join 

 将两个表做成分桶表，切分成多个小文件

拆开成小表

分桶数和第一张表的分桶数为倍数关系



### 数据倾斜

group by

数据倾斜

```sql
是否在 Map 端进行聚合，默认为 True //预聚合
set hive.map.aggr = true;
在 Map 端进行聚合操作的条目数目
set hive.groupby.mapaggr.checkinterval = 100000;
有数据倾斜的时候进行负载均衡（默认是 false）
set hive.groupby.skewindata = true; //先打散，再聚合
当选项设定为 true，生成的查询计划会有两个 MR Job
```



增加reducer的个数（多个key同时导致数据倾斜）

在 hadoop 的 mapred-default.xml 文件中修改设置每个 job 的 Reduce 个数

set mapreduce.job.reduces = 15;



通过抽样看是多个key导致还是单个key导致



```sql
#join 的键对应的记录条数超过这个值则会进行分拆，值根据具体数据量设置
set hive.skewjoin.key=100000;
# 如果是 join 过程出现倾斜应该设置为 true 
set hive.optimize.skewjoin=false;
```



## 架构说明

Spark on Hive ：数据源是：hive     Spark 获取hive中的数据，然后进行SparkSQL的操作  （hive只是作为一个spark的数据源）。

Hvie on Spark ：（数据源是hive本身）   Hvie 将自己的MapReduce计算引擎替换为Spark，当我们执行HiveSQL(HQL)时底层以经不是将HQL转换为MapReduce任务，而是跑的Spark任务（即：将HQL转换为Spark任务

## 事实表的分类

### 事务型事实表

以**每个事务或事件为单位**，例如一个销售订单记录，一笔支付记录等，作为事实表里的一行数据。一旦事务被提交，事实表数据被插入，数据就不再进行更改，其更新方式为增量更新。

### 周期型快照事实表

周期型快照事实表中**不会保留所有数据**，**只保留固定时间间隔的数据**，例如每天或者每月的销售额，或每月的账户余额等。

例如购物车，有加减商品，随时都有可能变化，但是我们更关心每天结束时这里面有多少商品，方便我们后期统计分析

### 累积型快照事实表

**累计快照事实表用于跟踪业务事实的变化。**例如，数据仓库中可能需要累积或者存储订单从下订单开始，到订单商品被打包、运输、和签收的各个业务阶段的时间点数据来跟踪订单声明周期的进展情况。当这个业务过程进行时，事实表的记录也要不断更新。



## 集群规模

集群规模：日活100w 每人一天平均100条 100w*100=1亿条

每条1k左右，100000000/1024/1024=100G     50、60G

ods lzo压缩35G  

DWD LZO+parquet列式存储 35G 

DWS 50G

ADS忽略不计

三副本 120G*3= 360G



kafka中数据



## dim 拉链表

```sql
with
tmp as
(
    select
        old.id old_id,
        old.login_name old_login_name,
        old.nick_name old_nick_name,
        old.name old_name,
        old.phone_num old_phone_num,
        old.email old_email,
        old.user_level old_user_level,
        old.birthday old_birthday,
        old.gender old_gender,
        old.create_time old_create_time,
        old.operate_time old_operate_time,
        old.start_date old_start_date,
        old.end_date old_end_date,
        new.id new_id,
        new.login_name new_login_name,
        new.nick_name new_nick_name,
        new.name new_name,
        new.phone_num new_phone_num,
        new.email new_email,
        new.user_level new_user_level,
        new.birthday new_birthday,
        new.gender new_gender,
        new.create_time new_create_time,
        new.operate_time new_operate_time,
        new.start_date new_start_date,
        new.end_date new_end_date
    from
    (
        select
            id,
            login_name,
            nick_name,
            name,
            phone_num,
            email,
            user_level,
            birthday,
            gender,
            create_time,
            operate_time,
            start_date,
            end_date
        from dim_user_info
        where dt='9999-99-99'
    )old
    full outer join
    (
        select
            id,
            login_name,
            nick_name,
            md5(name) name,
            md5(phone_num) phone_num,
            md5(email) email,
            user_level,
            birthday,
            gender,
            create_time,
            operate_time,
            '2020-06-15' start_date,
            '9999-99-99' end_date
        from ods_user_info
        where dt='2020-06-15'
    )new
    on old.id=new.id
)
insert overwrite table dim_user_info partition(dt)
select
    nvl(new_id,old_id),
    nvl(new_login_name,old_login_name),
    nvl(new_nick_name,old_nick_name),
    nvl(new_name,old_name),
    nvl(new_phone_num,old_phone_num),
    nvl(new_email,old_email),
    nvl(new_user_level,old_user_level),
    nvl(new_birthday,old_birthday),
    nvl(new_gender,old_gender),
    nvl(new_create_time,old_create_time),
    nvl(new_operate_time,old_operate_time),
    nvl(new_start_date,old_start_date),
    nvl(new_end_date,old_end_date),
    nvl(new_end_date,old_end_date) dt
from tmp
union all
select
    old_id,
    old_login_name,
    old_nick_name,
    old_name,
    old_phone_num,
    old_email,
    old_user_level,
    old_birthday,
    old_gender,
    old_create_time,
    old_operate_time,
    old_start_date,
    cast(date_add('2020-06-15',-1) as string),
    cast(date_add('2020-06-15',-1) as string) dt
from tmp
where new_id is not null and old_id is not null;
```

拉链表

记录每条数据的生命周期，一旦生命周期结束，开始一条新的记录

拉链表适合于，数据会发生变化，但是变化频率并不高的维度

startend enddate

第一次导入直接使用ods_user_info，集群投入使用的第一天作为start-date,9999作为结束日期

每日更新的话，就是由你从ods_user_info读取当日的数据，然后start_date就为当天日期,9999为结束日期，、

然后新表（从ods_user_info读取的数据）和旧表（dim_user_info）读取的数据关联

通过nvl语法保存了当天新增和修改的数据和过去的未修改的数据

还需要过去的修改的过期的数据，new_id is not null and old_id is not null,把他的结束日期设置为今天的前一天



## dwd订单事实表

首日装载

```sql
insert overwrite table dwd_order_info partition(dt)
select
    oi.id,
    oi.order_status,
    oi.user_id,
    oi.province_id,
    oi.payment_way,
    oi.delivery_address,
    oi.out_trade_no,
    oi.tracking_no,
    oi.create_time,
    times.ts['1002'] payment_time,
    times.ts['1003'] cancel_time,
    times.ts['1004'] finish_time,
    times.ts['1005'] refund_time,
    times.ts['1006'] refund_finish_time,
    oi.expire_time,
    feight_fee,
    feight_fee_reduce,
    activity_reduce_amount,
    coupon_reduce_amount,
    original_amount,
    final_amount,
    case
    --取消就完成
        when times.ts['1003'] is not null then date_format(times.ts['1003'],'yyyy-MM-dd')
    --7天没退款    
        when times.ts['1004'] is not null and date_add(date_format(times.ts['1004'],'yyyy-MM-dd'),7)<='2020-06-14' and times.ts['1005'] is null then date_add(date_format(times.ts['1004'],'yyyy-MM-dd'),7)
        when times.ts['1006'] is not null then date_format(times.ts['1006'],'yyyy-MM-dd')
        when oi.expire_time is not null then date_format(oi.expire_time,'yyyy-MM-dd')
        else '9999-99-99'
    end
from
(
    select
        *
    from ods_order_info
    where dt='2020-06-14'
)oi
left join
(
    select
        order_id,
        str_to_map(concat_ws(',',collect_set(concat(order_status,'=',operate_time))),',','=') ts
    from ods_order_status_log
    where dt='2020-06-14'
    group by order_id
)times
on oi.id=times.order_id;
```



获取订单各个生命周期的时间，需要采用订单状态流水表



concat拼接



使用collect set聚合变成一个数组

![1662556006484](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662556006484.png)

collcat_ws 加数组中的元素组合成一个字符串

![1662556101201](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662556101201.png)

str_to_map将字符串转换成map

![1662556164047](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662556164047.png)



每日

```sql
insert overwrite table dwd_order_info partition(dt)
select
    nvl(new.id,old.id),
    nvl(new.order_status,old.order_status),
    nvl(new.user_id,old.user_id),
    nvl(new.province_id,old.province_id),
    nvl(new.payment_way,old.payment_way),
    nvl(new.delivery_address,old.delivery_address),
    nvl(new.out_trade_no,old.out_trade_no),
    nvl(new.tracking_no,old.tracking_no),
    nvl(new.create_time,old.create_time),
    nvl(new.payment_time,old.payment_time),
    nvl(new.cancel_time,old.cancel_time),
    nvl(new.finish_time,old.finish_time),
    nvl(new.refund_time,old.refund_time),
    nvl(new.refund_finish_time,old.refund_finish_time),
    nvl(new.expire_time,old.expire_time),
    nvl(new.feight_fee,old.feight_fee),
    nvl(new.feight_fee_reduce,old.feight_fee_reduce),
    nvl(new.activity_reduce_amount,old.activity_reduce_amount),
    nvl(new.coupon_reduce_amount,old.coupon_reduce_amount),
    nvl(new.original_amount,old.original_amount),
    nvl(new.final_amount,old.final_amount),
    case
        when nvl(new.create_time,old.create_time) then date_format(new.cancel_time,'yyyy-MM-dd')
        when nvl(new.finish_time,old.finish_time), is not null and date_add(date_format(nvl(new.finish_time,old.finish_time),,'yyyy-MM-dd'),7)='2020-06-15' and nvl(new.refund_time,old.refund_time) is null then '2020-06-15'
        when nvl(new.refund_finish_time,old.refund_finish_time) is not null then date_format(new.refund_finish_time,'yyyy-MM-dd')
        when nvl(new.expire_time,old.expire_time) is not null then date_format(new.expire_time,'yyyy-MM-dd')
        else '9999-99-99'
    end
from
(
    select
        id,
        order_status,
        user_id,
        province_id,
        payment_way,
        delivery_address,
        out_trade_no,
        tracking_no,
        create_time,
        payment_time,
        cancel_time,
        finish_time,
        refund_time,
        refund_finish_time,
        expire_time,
        feight_fee,
        feight_fee_reduce,
        activity_reduce_amount,
        coupon_reduce_amount,
        original_amount,
        final_amount
    from dwd_order_info
    where dt='9999-99-99'
)old
full outer join
(
    select
        oi.id,
        oi.order_status,
        oi.user_id,
        oi.province_id,
        oi.payment_way,
        oi.delivery_address,
        oi.out_trade_no,
        oi.tracking_no,
        oi.create_time,
        times.ts['1002'] payment_time,
        times.ts['1003'] cancel_time,
        times.ts['1004'] finish_time,
        times.ts['1005'] refund_time,
        times.ts['1006'] refund_finish_time,
        oi.expire_time,
        feight_fee,
        feight_fee_reduce,
        activity_reduce_amount,
        coupon_reduce_amount,
        original_amount,
        final_amount
    from
    (
        select
            *
        from ods_order_info
        where dt='2020-06-15'
    )oi
    left join
    (
        select
            order_id,
            str_to_map(concat_ws(',',collect_set(concat(order_status,'=',operate_time))),',','=') ts
        from ods_order_status_log
        where dt='2020-06-15'
        group by order_id
    )times
    on oi.id=times.order_id
 )new
on old.id=new.id;
```



## dws用户主题

```sql
DROP TABLE IF EXISTS dws_user_action_daycount;
CREATE EXTERNAL TABLE dws_user_action_daycount
(
    `user_id` STRING COMMENT '用户id',
    `login_count` BIGINT COMMENT '登录次数',
    `cart_count` BIGINT COMMENT '加入购物车次数',
    `favor_count` BIGINT COMMENT '收藏次数',
    `order_count` BIGINT COMMENT '下单次数',
    `order_activity_count` BIGINT COMMENT '订单参与活动次数',
    `order_activity_reduce_amount` DECIMAL(16,2) COMMENT '订单减免金额(活动)',
    `order_coupon_count` BIGINT COMMENT '订单用券次数',
    `order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '订单减免金额(优惠券)',
    `order_original_amount` DECIMAL(16,2)  COMMENT '订单单原始金额',
    `order_final_amount` DECIMAL(16,2) COMMENT '订单总金额',
    `payment_count` BIGINT COMMENT '支付次数',
    `payment_amount` DECIMAL(16,2) COMMENT '支付金额',
    `refund_order_count` BIGINT COMMENT '退单次数',
    `refund_order_num` BIGINT COMMENT '退单件数',
    `refund_order_amount` DECIMAL(16,2) COMMENT '退单金额',
    `refund_payment_count` BIGINT COMMENT '退款次数',
    `refund_payment_num` BIGINT COMMENT '退款件数',
    `refund_payment_amount` DECIMAL(16,2) COMMENT '退款金额',
    `coupon_get_count` BIGINT COMMENT '优惠券领取次数',
    `coupon_using_count` BIGINT COMMENT '优惠券使用(下单)次数',
    `coupon_used_count` BIGINT COMMENT '优惠券使用(支付)次数',
    `appraise_good_count` BIGINT COMMENT '好评数',
    `appraise_mid_count` BIGINT COMMENT '中评数',
    `appraise_bad_count` BIGINT COMMENT '差评数',
    `appraise_default_count` BIGINT COMMENT '默认评价数',
    `order_detail_stats` array<struct<sku_id:string,sku_num:bigint,order_count:bigint,activity_reduce_amount:decimal(16,2),coupon_reduce_amount:decimal(16,2),original_amount:decimal(16,2),final_amount:decimal(16,2)>> COMMENT '下单明细统计'
) COMMENT '每日用户行为'
PARTITIONED BY (`dt` STRING)
STORED AS PARQUET
LOCATION '/warehouse/gmall/dws/dws_user_action_daycount/'
TBLPROPERTIES ("parquet.compression"="lzo");
```



维度：用户

|          度量值          |     来源表     |                                                              |
| :----------------------: | :------------: | ------------------------------------------------------------ |
|         登录次数         |  dwd_page_log  | ![1662552966972](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662552966972.png) |
| 加入购物车次数、收藏次数 | dwd_action_log | ![1662553055626](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662553055626.png) |
|         下单次数         | dwd_order_info | ![1662553121600](C:\Users\Lin\AppData\Roaming\Typora\typora-user-images\1662553121600.png) |



首日装载

```sql
with
tmp_login as
(
    select
        dt,
        user_id,
        count(*) login_count
    from dwd_page_log
    where user_id is not null
    and last_page_id is null
    group by dt,user_id
),
tmp_cf as
(
    select
        dt,
        user_id,
        sum(if(action_id='cart_add',1,0)) cart_count,
        sum(if(action_id='favor_add',1,0)) favor_count
    from dwd_action_log
    where user_id is not null
    and action_id in ('cart_add','favor_add')
    group by dt,user_id
),
tmp_order as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        user_id,
        count(*) order_count,
        sum(if(activity_reduce_amount>0,1,0)) order_activity_count,
        sum(if(coupon_reduce_amount>0,1,0)) order_coupon_count,
        sum(activity_reduce_amount) order_activity_reduce_amount,
        sum(coupon_reduce_amount) order_coupon_reduce_amount,
        sum(original_amount) order_original_amount,
        sum(final_amount) order_final_amount
    from dwd_order_info
    group by date_format(create_time,'yyyy-MM-dd'),user_id
),
tmp_pay as
(
    select
        date_format(callback_time,'yyyy-MM-dd') dt,
        user_id,
        count(*) payment_count,
        sum(payment_amount) payment_amount
    from dwd_payment_info
    group by date_format(callback_time,'yyyy-MM-dd'),user_id
),
tmp_ri as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        user_id,
        count(*) refund_order_count,
        sum(refund_num) refund_order_num,
        sum(refund_amount) refund_order_amount
    from dwd_order_refund_info
    group by date_format(create_time,'yyyy-MM-dd'),user_id
),
tmp_rp as
(
    select
        date_format(callback_time,'yyyy-MM-dd') dt,
        rp.user_id,
        count(*) refund_payment_count,
        sum(ri.refund_num) refund_payment_num,
        sum(rp.refund_amount) refund_payment_amount
    from
    (
        select
            user_id,
            order_id,
            sku_id,
            refund_amount,
            callback_time
        from dwd_refund_payment
    )rp
    left join
    (
        select
            user_id,
            order_id,
            sku_id,
            refund_num
        from dwd_order_refund_info
    )ri
    on rp.order_id=ri.order_id
    and rp.sku_id=rp.sku_id
    group by date_format(callback_time,'yyyy-MM-dd'),rp.user_id
),
tmp_coupon as
(
    select
        coalesce(coupon_get.dt,coupon_using.dt,coupon_used.dt) dt,
        coalesce(coupon_get.user_id,coupon_using.user_id,coupon_used.user_id) user_id,
        nvl(coupon_get_count,0) coupon_get_count,
        nvl(coupon_using_count,0) coupon_using_count,
        nvl(coupon_used_count,0) coupon_used_count
    from
    (
        select
            date_format(get_time,'yyyy-MM-dd') dt,
            user_id,
            count(*) coupon_get_count
        from dwd_coupon_use
        where get_time is not null
        group by user_id,date_format(get_time,'yyyy-MM-dd')
    )coupon_get
    full outer join
    (
        select
            date_format(using_time,'yyyy-MM-dd') dt,
            user_id,
            count(*) coupon_using_count
        from dwd_coupon_use
        where using_time is not null
        group by user_id,date_format(using_time,'yyyy-MM-dd')
    )coupon_using
    on coupon_get.dt=coupon_using.dt
    and coupon_get.user_id=coupon_using.user_id
    full outer join
    (
        select
            date_format(used_time,'yyyy-MM-dd') dt,
            user_id,
            count(*) coupon_used_count
        from dwd_coupon_use
        where used_time is not null
        group by user_id,date_format(used_time,'yyyy-MM-dd')
    )coupon_used
    on nvl(coupon_get.dt,coupon_using.dt)=coupon_used.dt
    and nvl(coupon_get.user_id,coupon_using.user_id)=coupon_used.user_id
),
tmp_comment as
(
    select
        date_format(create_time,'yyyy-MM-dd') dt,
        user_id,
        sum(if(appraise='1201',1,0)) appraise_good_count,
        sum(if(appraise='1202',1,0)) appraise_mid_count,
        sum(if(appraise='1203',1,0)) appraise_bad_count,
        sum(if(appraise='1204',1,0)) appraise_default_count
    from dwd_comment_info
    group by date_format(create_time,'yyyy-MM-dd'),user_id
),
tmp_od as
(
    select
        dt,
        user_id,
        collect_set(named_struct('sku_id',sku_id,'sku_num',sku_num,'order_count',order_count,'activity_reduce_amount',activity_reduce_amount,'coupon_reduce_amount',coupon_reduce_amount,'original_amount',original_amount,'final_amount',final_amount)) order_detail_stats
    from
    (
        select
            date_format(create_time,'yyyy-MM-dd') dt,
            user_id,
            sku_id,
            sum(sku_num) sku_num,
            count(*) order_count,
            cast(sum(split_activity_amount) as decimal(16,2)) activity_reduce_amount,
            cast(sum(split_coupon_amount) as decimal(16,2)) coupon_reduce_amount,
            cast(sum(original_amount) as decimal(16,2)) original_amount,
            cast(sum(split_final_amount) as decimal(16,2)) final_amount
        from dwd_order_detail
        group by date_format(create_time,'yyyy-MM-dd'),user_id,sku_id
    )t1
    group by dt,user_id
)
insert overwrite table dws_user_action_daycount partition(dt)
select
    coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id,tmp_ri.user_id,tmp_rp.user_id,tmp_comment.user_id,tmp_coupon.user_id,tmp_od.user_id),
    nvl(login_count,0),
    nvl(cart_count,0),
    nvl(favor_count,0),
    nvl(order_count,0),
    nvl(order_activity_count,0),
    nvl(order_activity_reduce_amount,0),
    nvl(order_coupon_count,0),
    nvl(order_coupon_reduce_amount,0),
    nvl(order_original_amount,0),
    nvl(order_final_amount,0),
    nvl(payment_count,0),
    nvl(payment_amount,0),
    nvl(refund_order_count,0),
    nvl(refund_order_num,0),
    nvl(refund_order_amount,0),
    nvl(refund_payment_count,0),
    nvl(refund_payment_num,0),
    nvl(refund_payment_amount,0),
    nvl(coupon_get_count,0),
    nvl(coupon_using_count,0),
    nvl(coupon_used_count,0),
    nvl(appraise_good_count,0),
    nvl(appraise_mid_count,0),
    nvl(appraise_bad_count,0),
    nvl(appraise_default_count,0),
    order_detail_stats,
    coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt,tmp_ri.dt,tmp_rp.dt,tmp_comment.dt,tmp_coupon.dt,tmp_od.dt)
from tmp_login
full outer join tmp_cf
on tmp_login.user_id=tmp_cf.user_id
and tmp_login.dt=tmp_cf.dt
full outer join tmp_order
on coalesce(tmp_login.user_id,tmp_cf.user_id)=tmp_order.user_id
and coalesce(tmp_login.dt,tmp_cf.dt)=tmp_order.dt
full outer join tmp_pay
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id)=tmp_pay.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt)=tmp_pay.dt
full outer join tmp_ri
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id)=tmp_ri.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt)=tmp_ri.dt
full outer join tmp_rp
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id,tmp_ri.user_id)=tmp_rp.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt,tmp_ri.dt)=tmp_rp.dt
full outer join tmp_comment
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id,tmp_ri.user_id,tmp_rp.user_id)=tmp_comment.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt,tmp_ri.dt,tmp_rp.dt)=tmp_comment.dt
full outer join tmp_coupon
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id,tmp_ri.user_id,tmp_rp.user_id,tmp_comment.user_id)=tmp_coupon.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt,tmp_ri.dt,tmp_rp.dt,tmp_comment.dt)=tmp_coupon.dt
full outer join tmp_od
on coalesce(tmp_login.user_id,tmp_cf.user_id,tmp_order.user_id,tmp_pay.user_id,tmp_ri.user_id,tmp_rp.user_id,tmp_comment.user_id,tmp_coupon.user_id)=tmp_od.user_id
and coalesce(tmp_login.dt,tmp_cf.dt,tmp_order.dt,tmp_pay.dt,tmp_ri.dt,tmp_rp.dt,tmp_comment.dt,tmp_coupon.dt)=tmp_od.dt;
```





dwt用户主题

```sql
DROP TABLE IF EXISTS dwt_user_topic;
CREATE EXTERNAL TABLE dwt_user_topic
(
    `user_id` STRING  COMMENT '用户id',
    `login_date_first` STRING COMMENT '首次活跃日期',
    `login_date_last` STRING COMMENT '末次活跃日期',
    `login_date_1d_count` STRING COMMENT '最近1日登录次数',
    `login_last_1d_day_count` BIGINT COMMENT '最近1日登录天数',
    `login_last_7d_count` BIGINT COMMENT '最近7日登录次数',
    `login_last_7d_day_count` BIGINT COMMENT '最近7日登录天数',
    `login_last_30d_count` BIGINT COMMENT '最近30日登录次数',
    `login_last_30d_day_count` BIGINT COMMENT '最近30日登录天数',
    `login_count` BIGINT COMMENT '累积登录次数',
    `login_day_count` BIGINT COMMENT '累积登录天数',
    `order_date_first` STRING COMMENT '首次下单时间',
    `order_date_last` STRING COMMENT '末次下单时间',
    `order_last_1d_count` BIGINT COMMENT '最近1日下单次数',
    `order_activity_last_1d_count` BIGINT COMMENT '最近1日订单参与活动次数',
    `order_activity_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日订单减免金额(活动)',
    `order_coupon_last_1d_count` BIGINT COMMENT '最近1日下单用券次数',
    `order_coupon_reduce_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日订单减免金额(优惠券)',
    `order_last_1d_original_amount` DECIMAL(16,2) COMMENT '最近1日原始下单金额',
    `order_last_1d_final_amount` DECIMAL(16,2) COMMENT '最近1日最终下单金额',
    `order_last_7d_count` BIGINT COMMENT '最近7日下单次数',
    `order_activity_last_7d_count` BIGINT COMMENT '最近7日订单参与活动次数',
    `order_activity_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日订单减免金额(活动)',
    `order_coupon_last_7d_count` BIGINT COMMENT '最近7日下单用券次数',
    `order_coupon_reduce_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日订单减免金额(优惠券)',
    `order_last_7d_original_amount` DECIMAL(16,2) COMMENT '最近7日原始下单金额',
    `order_last_7d_final_amount` DECIMAL(16,2) COMMENT '最近7日最终下单金额',
    `order_last_30d_count` BIGINT COMMENT '最近30日下单次数',
    `order_activity_last_30d_count` BIGINT COMMENT '最近30日订单参与活动次数',
    `order_activity_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日订单减免金额(活动)',
    `order_coupon_last_30d_count` BIGINT COMMENT '最近30日下单用券次数',
    `order_coupon_reduce_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日订单减免金额(优惠券)',
    `order_last_30d_original_amount` DECIMAL(16,2) COMMENT '最近30日原始下单金额',
    `order_last_30d_final_amount` DECIMAL(16,2) COMMENT '最近30日最终下单金额',
    `order_count` BIGINT COMMENT '累积下单次数',
    `order_activity_count` BIGINT COMMENT '累积订单参与活动次数',
    `order_activity_reduce_amount` DECIMAL(16,2) COMMENT '累积订单减免金额(活动)',
    `order_coupon_count` BIGINT COMMENT '累积下单用券次数',
    `order_coupon_reduce_amount` DECIMAL(16,2) COMMENT '累积订单减免金额(优惠券)',
    `order_original_amount` DECIMAL(16,2) COMMENT '累积原始下单金额',
    `order_final_amount` DECIMAL(16,2) COMMENT '累积最终下单金额',
    `payment_date_first` STRING COMMENT '首次支付时间',
    `payment_date_last` STRING COMMENT '末次支付时间',
    `payment_last_1d_count` BIGINT COMMENT '最近1日支付次数',
    `payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日支付金额',
    `payment_last_7d_count` BIGINT COMMENT '最近7日支付次数',
    `payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日支付金额',
    `payment_last_30d_count` BIGINT COMMENT '最近30日支付次数',
    `payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日支付金额',
    `payment_count` BIGINT COMMENT '累积支付次数',
    `payment_amount` DECIMAL(16,2) COMMENT '累积支付金额',
    `refund_order_last_1d_count` BIGINT COMMENT '最近1日退单次数',
    `refund_order_last_1d_num` BIGINT COMMENT '最近1日退单件数',
    `refund_order_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退单金额',
    `refund_order_last_7d_count` BIGINT COMMENT '最近7日退单次数',
    `refund_order_last_7d_num` BIGINT COMMENT '最近7日退单件数',
    `refund_order_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退单金额',
    `refund_order_last_30d_count` BIGINT COMMENT '最近30日退单次数',
    `refund_order_last_30d_num` BIGINT COMMENT '最近30日退单件数',
    `refund_order_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退单金额',
    `refund_order_count` BIGINT COMMENT '累积退单次数',
    `refund_order_num` BIGINT COMMENT '累积退单件数',
    `refund_order_amount` DECIMAL(16,2) COMMENT '累积退单金额',
    `refund_payment_last_1d_count` BIGINT COMMENT '最近1日退款次数',
    `refund_payment_last_1d_num` BIGINT COMMENT '最近1日退款件数',
    `refund_payment_last_1d_amount` DECIMAL(16,2) COMMENT '最近1日退款金额',
    `refund_payment_last_7d_count` BIGINT COMMENT '最近7日退款次数',
    `refund_payment_last_7d_num` BIGINT COMMENT '最近7日退款件数',
    `refund_payment_last_7d_amount` DECIMAL(16,2) COMMENT '最近7日退款金额',
    `refund_payment_last_30d_count` BIGINT COMMENT '最近30日退款次数',
    `refund_payment_last_30d_num` BIGINT COMMENT '最近30日退款件数',
    `refund_payment_last_30d_amount` DECIMAL(16,2) COMMENT '最近30日退款金额',
    `refund_payment_count` BIGINT COMMENT '累积退款次数',
    `refund_payment_num` BIGINT COMMENT '累积退款件数',
    `refund_payment_amount` DECIMAL(16,2) COMMENT '累积退款金额',
    `cart_last_1d_count` BIGINT COMMENT '最近1日加入购物车次数',
    `cart_last_7d_count` BIGINT COMMENT '最近7日加入购物车次数',
    `cart_last_30d_count` BIGINT COMMENT '最近30日加入购物车次数',
    `cart_count` BIGINT COMMENT '累积加入购物车次数',
    `favor_last_1d_count` BIGINT COMMENT '最近1日收藏次数',
    `favor_last_7d_count` BIGINT COMMENT '最近7日收藏次数',
    `favor_last_30d_count` BIGINT COMMENT '最近30日收藏次数',
    `favor_count` BIGINT COMMENT '累积收藏次数',
    `coupon_last_1d_get_count` BIGINT COMMENT '最近1日领券次数',
    `coupon_last_1d_using_count` BIGINT COMMENT '最近1日用券(下单)次数',
    `coupon_last_1d_used_count` BIGINT COMMENT '最近1日用券(支付)次数',
    `coupon_last_7d_get_count` BIGINT COMMENT '最近7日领券次数',
    `coupon_last_7d_using_count` BIGINT COMMENT '最近7日用券(下单)次数',
    `coupon_last_7d_used_count` BIGINT COMMENT '最近7日用券(支付)次数',
    `coupon_last_30d_get_count` BIGINT COMMENT '最近30日领券次数',
    `coupon_last_30d_using_count` BIGINT COMMENT '最近30日用券(下单)次数',
    `coupon_last_30d_used_count` BIGINT COMMENT '最近30日用券(支付)次数',
    `coupon_get_count` BIGINT COMMENT '累积领券次数',
    `coupon_using_count` BIGINT COMMENT '累积用券(下单)次数',
    `coupon_used_count` BIGINT COMMENT '累积用券(支付)次数',
    `appraise_last_1d_good_count` BIGINT COMMENT '最近1日好评次数',
    `appraise_last_1d_mid_count` BIGINT COMMENT '最近1日中评次数',
    `appraise_last_1d_bad_count` BIGINT COMMENT '最近1日差评次数',
    `appraise_last_1d_default_count` BIGINT COMMENT '最近1日默认评价次数',
    `appraise_last_7d_good_count` BIGINT COMMENT '最近7日好评次数',
    `appraise_last_7d_mid_count` BIGINT COMMENT '最近7日中评次数',
    `appraise_last_7d_bad_count` BIGINT COMMENT '最近7日差评次数',
    `appraise_last_7d_default_count` BIGINT COMMENT '最近7日默认评价次数',
    `appraise_last_30d_good_count` BIGINT COMMENT '最近30日好评次数',
    `appraise_last_30d_mid_count` BIGINT COMMENT '最近30日中评次数',
    `appraise_last_30d_bad_count` BIGINT COMMENT '最近30日差评次数',
    `appraise_last_30d_default_count` BIGINT COMMENT '最近30日默认评价次数',
    `appraise_good_count` BIGINT COMMENT '累积好评次数',
    `appraise_mid_count` BIGINT COMMENT '累积中评次数',
    `appraise_bad_count` BIGINT COMMENT '累积差评次数',
    `appraise_default_count` BIGINT COMMENT '累积默认评价次数'
)COMMENT '会员主题宽表'
PARTITIONED BY (`dt` STRING)
STORED AS PARQUET
LOCATION '/warehouse/gmall/dwt/dwt_user_topic/'
TBLPROPERTIES ("parquet.compression"="lzo");
```

